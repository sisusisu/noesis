<?xml version="1.0"  encoding="ISO-8859-1" ?> 
<!DOCTYPE sys-ents [ 
  <!ENTITY bibliography   SYSTEM "bibliography.xml">
]> 
<?xml-stylesheet type="text/xsl" href="../book.xsl"?>

<!-- Graph exploration -->

<document>
&bibliography;
<tag>graph-exploration</tag>
<title>Graph exploration</title>

<text>
A fundamental graph problem: Visiting every node (or link) in a network in a systematic way, i.e. graph traversal.
</text>

<text>
++ CS
Graph-searching procedures such as depth-first search (DFS)
<cite>Tarjan 1972</cite> <cite>Hopcroft and Tarjan 1973</cite>
and breadth-first search (BFS) 
<cite>Moore 1959</cite>
form the basic preprocessing steps for most graph algorithms. 
</text>

<text>
For efficiency, we should not visit the same node repeatedly. For correctness, we should guarantee that every node is eventually visited. IDEA: Keep track of which nodes we have already visited...
</text>


<text>
BFS/DFS define a tree on the vertices of the graph. 
+ Actually, for arbitrary graphs, which might not be connected, DFS/BFS return a forest of trees, with each tree in the forest containing the vertices that belong to each connected component of the graph.
</text>

<note>
<title>The Visitor design pattern</title>

<text>
++ Visitor design pattern <cite>Gamma et al. 1994</cite>
</text>

</note>


<document>
<tag>graph-exploration-dfs</tag>
<title>Depth-first search</title>

<text>
Implementation: recursion/stack (LIFO: we explore a path until we meet a dead end (i.e. no unvisited neighbors) and then backtrack). Recursion eliminates the need of an explicit stack and provides a neat implementation of the algorithm.
</text>


<text>
EFFICIENCY: <eqn>O(n+m)</eqn> using adjacency lists
</text>



<text>
Time intervals: a clock ticks every time we enter or exit a node and we keep track of the entry and exit times for each node.
</text>

<text>
Properties:

- Time intervals are properly nested: if we enter x before y, we exit y before x; i.e. all nodes reachable from a given node are explored before we finish with it.

- The difference between exit and entry times indicate the number of descendants of any given node. Actually half the time difference is the number of descendants in the search tree.

- For undirected graphs, DFS partitions edges into tree edges and back edges. Tree edges appear in the DFS tree, whereas back edges always point to an ancestor in the DFS tree (never to a sibling nor cousin node, just because all nodes reachable from a given node are explored before we finish with it).
</text>

<!--
Classifying (x,y) edges:

- parent[y] == x  => Tree edge

- discovered[y] and not processed[y]  => Back edge

For directed graphs:

- discovered[y] and (entry_time[y] &gt; entry_time[x]) => Forward edge

- discovered[y] and (entry_time[y] &lt; entry_time[x]) => Cross edge
-->


<text>
APPLICATIONS
</text>

<text>
++ CS        Algorithms based on DFS have been known for a long time for the problem of searching
mazes. However, it was the work of Hopcroft and Tarjan (for which they
received the ACM Turing Award in 1986) that illustrated the full algorithmic
power of DFS <cite>Hopcroft and Tarjan 1973</cite>. They demonstrated efficient algorithms for several problems,
such as finding biconnected components and bridges of a graph and testing
triconnectivity and planarity. DFS on directed graphs can be used to
classify its vertices into strongly connected components, to detect cycles,
and to find a topological order of the vertices of a DAG.
</text>


<text>
Solving mazes (e.g. <cite>Moore 1959</cite>): A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Tremaux as a strategy for solving mazes.
</text>

<text>
ON UNDIRECTED GRAPHS
</text>

<text>
++ Connected components: Identify the different contextec components of a graph G, where x and y are members of different components if no path exists from x to y in G. O(m+n)
</text>


<text>
++ Cycle detection / tree testing: If there are no back edges, all edges belong to the DFS tree, hence the graph is actually a tree. When present, the back edge and the DFS tree define the cycle.
</text>

<text>
++ Graph biconnectedness: Articulation vertices (a.k.a. cut-nodes): Vertices whose deletion disconnects the graph connected component (i.e. single points of failure). Graphs without articulation points are said to be biconnected, since they have no single points of failure. A brute force algorithm would remove each vertex and check whether the graph is still connected (an <eqn>O(n(n+m))</eqn> algorithm), albeit a linear algorithm is possible based on DFS if we track the earliest reachable node for each node. If the DFS tree represented the whole graph, all internal nodes would be articulation points. Three situations have to be considered <cite>Skiena 2008</cite>: 

1) root cut-nodes (if the root of the DFS tree has more than one child, then it is an articulation point), 

2) bridge cut-nodes (if v is an internal node in the DFS tree and its earliest reachable node is v, then v is an articulation point)

3) parent cut-nodes (if v is an internal node in the DFS tree and its earliest reachable node is its parent in the tree, then the parent is an articulation point unless it is the root of the DFS tree, i.e. it cuts v and its descendants from the graph).
</text>

<text>
++ Edge biconnectedness: Bridges (in terms of link failures instead of node failures). A single edge whose deletion disconnects the graph is called a bridge. Any graph without bridges is said to be edge-biconnected. Edge (x,y) is a bridge when it is a DFS tree edge and no back edge connects from y or below to x or above.
</text>


<text>
The classic sequential algorithm for computing biconnected components in a connected undirected graph due to John Hopcroft and Robert Tarjan (1973) runs in linear time, and is based on depth-first search <cite>Hopcroft and Tarjan 1973</cite>

In the online version of the problem, vertices and edges are added (but not removed) dynamically, and a data structure must maintain the biconnected components. <cite>Westbrook and Tarjan 1992</cite> developed an efficient data structure for this problem based on disjoint-set data structures. Specifically, it processes n vertex additions and m edge additions in <eqn>O(m \alpha(m, n))</eqn> total time, where alpha is the inverse Ackermann function. This time bound is proved to be optimal.

<cite>Tarjan and Vishkin 1985</cite> designed a parallel algorithm on CRCW PRAM that runs in O(log n) time with n + m processors. 

<cite>Cong and Bader 2005</cite> developed an algorithm that achieves a speedup of 5 with 12 processors on SMPs.
</text>


<text>
ON DIRECTED GRAPHS
</text>

<text>
++ Cycle detection / DAG testing: If there are no back edges, all edges belong to the DFS tree, hence the graph is actually a DAG. As above, when present, the back edge and the DFS tree define the cycle.
</text>

<text>
++ Topological sorting: A common operation on directed acyclic graphs. Finding a linear ordering of the vertices so that for each arc (i,j), vertex i is to the left of vertex j (such ordering cannot exist if the graph contains a directed cycle, i.e. no back edges). Each DAG has, at least, one topological sort (often, many different ones, up to n! if there are no constraints), which gives an ordering to process each vertex before any of its successors (e.g. precedence constraints). Only DAGs can be topologically sorted. 

1) Considering the nodes in the reverse order they are processed by DFS returns a valid topological sort.

2) Using a DFS to identify all source vertices (of in-degree 0), which can appear at the start of the topological sort without violating any constraints. Deleting outgoing edges of the source vertices creates new source vertices. The process is repeated until all vertices are included in the topological sort. Proper use of data structures is sufficient to obtain a linear time O(n+m) algorithm.

 ref. first described by <cite>Kahn 1962</cite>: <url>http://en.wikipedia.org/wiki/Topological_sorting</url>
 Applications: scheduling tasks under precedence constraints (any topological sort, a.k.a. linear extension, defines an order to perform the tasks that satisfies all precedence constraints).


TOPOLOGICAL SORT

Reading: CLRS 22.4

Absent-minded Mr. Sandler (like professor Budimlic before him) has a problem when getting ready to go to work in the morning: he sometimes dresses out of order. For example, he might put his shoes on before putting the socks on, so he’ll have to take the shoes off, put the socks on and than the shoes back on. There’s also a shirt, tie, belt, shorts, pants, watch and jacket that have to be put on in a certain order. (There will be no in-class demo.)



The order between different parts of clothing forms a graph: shorts before pants means there’s an edge between shorts and pants. (We generally call a graph like this a dependancy graph, because it describes dependencies between pairs of tasks.)

Let’s do a DFS of this graph. The discovery and finishing times (d/f; see the lecture on DFS) might be:

Shorts: (1/10), pants(2/9), belt(3/6), jacket(4/5), shoes (7/8), socks (11/12), tie(13/14), watch(15/16), shirt(17/18).
If we order the nodes based on the decreasing finishing time of the DFS, we get: shirt, watch, tie, socks, shorts, pants, shoes, belt, jacket. (This is an order that is sure to get Mr. Sandler dressed without any time-consuming backtracking.)

This is called topological sort. It only makes sense for directed acyclic graphs. If the graph is cyclic, no topological order exists (because something always depends on something else). The topological order is a linear order of vertices such that if there exists an edge (u,v) ? G, vertex u appears before v in the ordering.

Here’s some pseudocode for an implementation of topological sort:

TOPOLOGICAL-SORT(G):
  call DFS(G) to compute f[v] for each vertex v in G;
  as each vertex v is finished, and f[v] computed,
      put v on the front of a linked list;
  return the linked list of vertices.

Lemma 1. A directed graph G is acyclic if and only if a DFS of G yields no back edges.

Proof:

= &gt;: If there is a back edge (u,v) then v is an anchestor of u in the DF forest, so there is a path from v to u, which with the (u,v) edge forms a cycle.

&lt; =: Suppose G has a cycle. Let v be the first vertex on the cycle that is discovered in the DFS. Let (u,v) be the incoming edge in the cycle. Since u is not yet discovered, there is a white-path from v to u at this point. According to the white-path theorem (see the DFS notes), u will be v’s descendant in the DF forest. Thus, (u,v) will become a back edge.

Theorem 2. TOPOLOGICAL-SORT(G) produces a topological sort of a directed acyclic graph G.

Proof: It is enough to show that for every edge (u,v) in G, f[u] > f[v]. Let’s look at the time d[u]: at this point, v cannot be gray, since then (u,v) would be a back edge, and G cyclic. v therefore has to be black or white. If v is white, than v is a descendant of u in the DF forest, and by parenthesis theorem f[u] > f[v]. If v is black, that means that v is already finished, so f[v] &lt; d[u] &lt; f[u].

</text>


<text>
++ Identifying weakly-connected components: Just ignore edge direction.
</text>

<text>
++ Identifying strongly-connected components (first algorithm by <cite>Tarjan 1972</cite>): 1) Traverse the graph atarting from any given node to discover the nodes that are reachable from the node. 2) Build a graph G'=(V,E') with the same vertex set but all its arcs reversed; i.e. <eqn>(y,z) \in E' iff (x,y) \in E</eqn>. 3) Perform a traveral starting from v in G', which will result in discovering the set of nodes that can reach v. 

If the second traversal does not completely traverse G', each DFS performed on G' will correspond to a strongly-connected component.

</text>

<text>
Any graph can be partitioned into a set of strongly-connected components. Using DFS, we can easily identify cycles. If we take into account that all the nodes involved in a cycle must belong to the same strongly-connected component, we can collapse the nodes in the cycle into a single vertex and repeat the process. When no cycles remain, each vertex represents a different strongly-connected component.
</text>

<text>
Linear SCC algorithms:

- Kosaraju's algorithm, 1978: <url>http://en.wikipedia.org/wiki/Kosaraju%27s_algorithm</url>
   ref.  <cite>Aho et al. 1983</cite> credit it to an unpublished paper from 1978 by S. Rao Kosaraju and Micha Sharir. (Al

- Cheriyan-Mehlhorn/Gabow algorithm, 1996/1999: <url>http://en.wikipedia.org/wiki/Cheriyan%E2%80%93Mehlhorn/Gabow_algorithm</url>
   ref. <cite>Cheriyan and Mehlhorn 1996</cite> <cite>Gabow 2003</cite>

- Tarjan's strongly connected components algorithm, 1972: <url>http://en.wikipedia.org/wiki/Tarjan's_strongly_connected_components_algorithm</url>
   ref. <cite>Tarjan 1972</cite>
   


 all efficiently compute the strongly connected components of a directed graph, but Tarjan's and Gabow's are favoured in practice since they require only one depth-first search rather than two. <url>http://en.wikipedia.org/wiki/Strongly_connected_component</url>


STRONGLY CONNECTED COMPONENTS

Reading: CLRS 22.5

Many algorithms need a decomposition of a directed graph into strongly connected components. A strongly connected componentof a directed graph G is a maximal subset G' ? G such that there is a path in G' from every vertex to every other vertex in G'. For example, many compiler algorithms decompose the programs into strongly connected components (loops) and optimize the components independently.

Here’s an example of a graph, with strongly connected components indicated. The nodes also have their d[v] and f[v] times indicated from a DFS.



Now for a bit of notation: We say GT, the transpose of G, is the graph G with all its edges reversed in direction.

Note that graphs G = (V,E) and GT = (V,ET) have identical sets of strongly connected components. (Why?)

How do you construct GT? If the representation is an adjacency matrix, just compute the transpose of that matrix. That’s O(|V|2) time (It can be O(1) time if you don’t need G any more: just invert the meaning of the matrix dimensions). If the representation is adjacency lists, then for every vertex v, for every vertex u in v’s adjacency list, add v to u’s adjacency list in GT Cost: O(E+V).

A component graph GSCC is a graph in which all the SCC are collapsed into single nodes, and edges between SCCs are maintained. For the graph above, the component graph looks like this:



We compute SCC by doing two DF traversals: first on G, and the second on GT, but in order of decreasing f[v] computed by first DFS.

STRONGLY-CONNECTED-COMPONENTS(G):
  call DFS(G) to compute f[v] for each vertex v
  compute GT
  call DFS(GT), but in the main loop of DFS,
      consider the vertices in order of decreasing f[v]
  output the vertices of each tree in the new DFS forest as a separate SCC
To show that the SCC algorithm computes SCCs, we need to make some key observations:

Lemma 3: GSCC is a DAG.

Proof: If there’s a path from C1 to C2 (meaning there is a path from some vertex u1 ? C1 to some vertex u2 in C2), there cannot be a path from C2 to C1 (a path from v2 in C2 to v1 in C1) , since that would imply that there is a path from u2 to v2, a path from v2 to v1 and a path from v1 to u1, meaning that there is a path from u2 to u1, contradicting the premise that C1 and C2 are distinct SCCs.

Since GSCC is a DAG, it can be topologically sorted. If we do a DFS on the topologically sorted GSCC in reverse order of the topologically sorted SCCs, then all the edges in GSCC will be cross edges, meaning that each SCC will be a tree in the second DFS.

We define f(C), where C is a subset of V, as max(f[v] | v ? C).

There are a couple of observations that formalize this intuition:

Lemma 4: Let C and C' be distinct SCCs in directed graph G = (V,E). Suppose there is an edge (u,v) in E, where u is in C and v is in C'. Then f(C) &gt; f(C').

Corollary 5: Suppose that there is an edge (u,v) in ET, where u is in C and v is in C'. Then f(C) &lt; f(C').

Latest corollary gives an insight of why the algorithms works. We start the second DFS with the SCC C that has the highest f(C). By Corollary 5, there are no edges going from C to any other strongly connected components. All vertices in C are reachable from the start, and all are on white paths, so they will all be on the DF tree. Thus, the first DF tree we produce will be the first SCC.

When we start the DFS on a strongly connected component C later in the algorithm, f(C) &gt; f(C') for all other C' that have yet to be visited (that’s how we choose roots for the second DFS). All the vertices in the C are on a white path from the root, so they will all be on a DF tree. Also, all the edges coming out of C will be the edges to the SCCs that are already visited (by Corollary 5), which means that no vertices other than the ones in C will be on that tree. So, the DF tree created at that point is an SCC.

The roots for the DF trees in the second DFS in our example graph will be b, c, g and h, in that order, and the discovered DF trees will be bae, cd, gf and h, which are the SCCs.


</text> 

<!-- Parallelization -->

<text>
Parallelization of DFS 
- for dense graphs @ Section 10.6 <cite>Grama et al. 2003</cite>
- Parallel DFS @ Section 11.4  <cite>Grama et al. 2003</cite>
</text>

<text>
An algorithm based on a depth-first search, such as those employed to discover the connected components of a network, can be parallelized by partitioning the adjacency matrix into p parts and assigning each part to a different processor. Each processor <eqn>P_i</eqn> will store a subgraph <eqn>G_i</eqn> of <eqn>G</eqn>. This subgraph <eqn>G_i=(V,E_i)</eqn> will contain the edges <eqn>E_i</eqn> included in the part of the adjacency matrix assigned to the processor. A parallel algorithm that computes the connected component of a network can be devised using a MergeSort-like divide-and-conquer strategy:
</text>

<list>

<item>Divide: Each processor computes the depth-first spanning forest of the graph <eqn>G_i</eqn>. At the end of this phase, <eqn>p</eqn> spanning forests have been obtained.</item>

<item>Conquer: Spanning forests are merged pairwise until a single spanning forest remains. In this forest, two vertices will be in the same connected component if and only if they are in the same tree.</item>

</list>


<text>
Partitioning the adjacency matrix and distributing it among p processors can be performed by block-striped mapping (each processor is assigned <eqn>n/p</eqn> consecutive rows or columns) or block-checkerboard mapping (the matrix is divided into <eqn>p</eqn> squares of size <eqn>n/\sqrt{p} \times n/\sqrt{p}</eqn> and each square is assigned to a different processor).
</text>


<text>
Merging pairs of forests requires combining disjoint sets of edges. An union-find data structure can be used to perform an efficient merge:

- find(x) returns the representative element of the set containing x.

- union(x,y) merges the sets containing the elements x and y.

Given two spanning forests to be merged, at most n-1 edges have to be merged (since they are forests spanning n nodes at most and not arbitrary graphs). For each edge (x,y) in the first forest, a find operation is performed to determine whether both vertices already belong to the same tree in the second forest. If not, the two trees in the second forest should be merged using an union operation. In the worst case, merging two forest requires 2(n-1) find operations and (n-1) union operations. Using an efficient implementation of the union-find data structure, e.g. by employing disjoint-set forests with ranking and path compression, all the required find and union operations can be performed in <eqn>O(n)</eqn> time.
</text>


<text>
Let us consider a p-processor message-passing computer:

- Computing the spanning forest from the <eqn>(n/p) \times n</eqn> adjacency matrix assigned to each processor requires <eqn>\Theta(n^2/p)</eqn> time.

- Merging all the <eqn>p</eqn> spanning forests requires <eqn>\log p</eqn> merging stages, each one taking <eqn>\Theta(n)</eqn> time. Therefore, the computational cost of merging is <eqn>\Theta(n \log p)</eqn>. In each merging process, spanning forests must be sent across processors. Since <eqn>\Theta(n)</eqn> edges must be transmitted, the overall communication cost of the merging algorithm is <eqn>\Theta(n \log p)</eqn>

The parallel run time of the resulting parallel algorithm is
</text>

<equation>
t_p \in \Theta \left ( \frac{n^2}{p} + n \log p \right ) 
</equation>

<text>
Since the sequential complexity of the algorithm is <eqn>W \in \Theta(n^2)</eqn> for dense networks, the speedup and efficiency of the parallel algorithm are
</text>

<equation>
S \in  \Theta \left ( \frac{n}{n/p + \log p} \right ) 
</equation>

<equation>
E = \frac{1}{1 + \Theta  ( (p \log p) / n ) }
</equation>

<text>
This algorithm is cost-optimal for <eqn>(p \log p) / n \in O(1)</eqn>, hence the parallel implementation can use <eqn>O(n / \log n)</eqn> different processors. Its isoefficiency function is <eqn>\Theta(p^2 \log p^2)</eqn>, which is due to communication costs in the merging phase (the isoefficiency function due to concurrency is <eqn>\Theta(n^2)</eqn>).
</text>

tp = n^2/p + n log(p)
ts = n^2 = W

to = p*tp - W = n^2 + np log(p) - n^2 = n p log(p)

E = 1 / (1 + to/W)

Communication overhead: p^2 log^2(p)
	tc = n p log (p)
	W  = n^2
	W = K tc  ===  n^2 = n p log(p)  ===  n = p log(p)  === W=n^2= p^2 log^2(p)
+
Concurrency: p^2
	Since n must grow at least as fast as p... W = n^2 = p^2 
=
Isoefficiency: \Theta(p^2 \log p^2)

NOTE: The performance of this parallel formulation is similar to that of Prim's minimum spanning tree algorithm and Dijkstra's single-source shortest paths algorithm on a message-passing computer.



</document>



<document>
<tag>graph-exploration-bfs</tag>
<title>Breadth-first search</title>

<text>
L: From a given starting node, breadth-first search visits its immediate neighbors first. Then the immediate neighbors of neighbors, and so on, until either reaching a node that has already been visited or reaching a node that has no other neighbors. Breadth-first search of an arbitrary network organizes surrounding nodes into levels, so the algorithm simply finds the smallest number of hops from a starting node v to the remaining nodes in the network. The level number of node w, relative to v, equals the number of hops from v to w. 
</text>

<text>
Implementation: queue (FIFO: we explore the oldest unexplored vertices first).
</text>

<text>
EFFICIENCY: <eqn>O(n+m)</eqn> using adjacency lists
</text>

<text>
Properties (due to the fact that each path in the tree must be the shortest path in the graph):

- For undirected graphs, edges not appearing in the breadth-first search tree can only point to nodes on the same level or to the level directly below.
</text>



<text>
APPLICATIONS
</text>

<text>
++ Shortest paths can be found by performing a breadth-first search on unweighted/binary graphs <cite>Moore 1959</cite>


(more elaborate algorithms are required for weighted graphs, as we will see later): the tree resulting from BFS defines the shortest paths from the root to the remaining nodes in the graph. IMPLEMENTATION: parent[node] @ Visitor + reversal using recursion/stack
</text>

<text>
++ Connected components: Any node we visit is part of the same connected component. Starting the search from any unvisited node, we can obtain additional connected components.
</text>

<text>
++ Two-coloring graphs = Bipartite testing: A graph is bipartite if it can be colored without conflicts using two colors. Whenever we visit a new node, we color it using the opposite of its parent's color. If edges appear with the same color at both ends, the graph is not bipartite. Otherwise, we have partitioned the graph.


"Color the first vertex blue, and then do a depth-first search of the graph. Whenever we discover a new, uncolored
vertex, color it opposite of its parent, since the same color would cause a clash. The graph cannot be bipartite if we ever find an edge (x, y) where both x and y have been colored identically. Otherwise, the final coloring will be a 2-coloring, constructed in O(n + m) time."
</text>









<text>
++ Distributed algorithm @ MIT Lecture 2 (synchronous) + Lecture 9 (asynchronous) / Section 4.2 + Section 15.4 <cite>Lynch 1997</cite>
@ MIT Distributed algorithms Lecture 21
</text>

<text>
++ Sublinear approximate algorithm @ MIT Sublinear algorithms => Goal: Quickly distinguish inputs that have specific property from those that are far from having the property...
</text>


</document>



<document>
<tag>graph-exploration-best-first</tag>
<title>Best-first search</title>

<text>
Best-first search... heuristics

Judea Pearl described best-first search as estimating the promise of node n by a <q>heuristic evaluation function f(n) which, in general, may depend on the description of n, the description of the goal, the information gathered by the search up to that point, and most important, on any extra knowledge about the problem domain</q> <cite>Pearl 1984</cite>.

</text>

<text>
<url>http://en.wikipedia.org/wiki/A*_search_algorithm</url>
A* search algorithm

HISTORY: In 1964 Nils Nilsson invented a heuristic based approach to increase the speed of Dijkstra's algorithm. This algorithm was called A1. In 1967 Bertram Raphael made dramatic improvements upon this algorithm, but failed to show optimality. He called this algorithm A2. Then in 1968 Peter E. Hart introduced an argument that proved A2 was optimal when using a consistent heuristic with only minor changes. His proof of the algorithm also included a section that showed that the new A2 algorithm was the best algorithm possible given the conditions. He thus named the new algorithm in Kleene star syntax to be the algorithm that starts with A and includes all possible version numbers or A*. <cite>Hart et al. 1968</cite> <cite>Hart et al. 1972</cite>

Often used for path finding...

++ Admissible heuristics...
</text>


<text>
<url>http://en.wikipedia.org/wiki/B*</url>
B* algorithm <cite>Berliner 1979</cite>
</text>


<text>
++ Parallel best-first search @ Section 11.5 <cite>Grama et al. 2003</cite>
</text>

</document>



<document>
<tag>graph-algorithm-example</tag>
<title>An easy graph problem: Maximal independent sets</title>

<text>
@ 10.7.1 <cite>Grama et al. 2003</cite>

Consider the problem of finding a maximal independent set (MIS) of vertices of a graph. We are given a sparse undirected graph G = (V, E). A set of vertices I \in V is called independent if no pair of vertices in I is connected via an edge in G. An independent set is called maximal if by including any other vertex not in I, the independence property is violated. 
+ Maximal independent sets are not unique. 
+ Maximal independent sets of vertices can be used to determine which computations can be done in parallel in certain types of task graphs.


Many algorithms have been proposed for computing a maximal independent set of vertices:

1) Greedy algorithm: 

The simplest class of algorithms starts by initially setting I to be empty, and assigning all vertices to a set C that acts as the candidate set of vertices for inclusion in I. Then the algorithm proceeds by repeatedly moving a vertex v from C into I and removing all vertices adjacent to v from C. This process terminates when C becomes empty, in which case I is a maximal independent set. The resulting set I will contain an independent set of vertices, because every time we add a vertex into I we remove from C all the vertices whose subsequent inclusion will violate the independence condition. Also, the resulting set is maximal, because any other vertex that is not already in I is adjacent to at least one of the vertices in I.

2) Luby's randomized algorithm: 

Even though the above algorithm is very simple, it is not well suited for parallel processing, as it is serial in nature. For this reason parallel MIS algorithms are usually based on the randomized algorithm originally developed by Luby for computing a coloring of a graph. Using Luby's algorithm, a maximal independent set I of vertices V a graph is computed in an incremental fashion as follows. 
  - The set I is initially set to be empty, and the set of candidate vertices, C, is set to be equal to V.
  - A unique random number is assigned to each vertex in C, and if a vertex has a random number that is smaller than all of the random numbers of the adjacent vertices, it is included in I.
  - The set C is updated so that all the vertices that were selected for inclusion in I and their adjacent vertices are removed from it. Note that the vertices that are selected for inclusion in I are indeed independent (i.e., not directly connected via an edge). This is because, if v was inserted in I, then the random number assigned to v is the smallest among the random numbers assigned to its adjacent vertices; thus, no other vertex u adjacent to v will have been selected for inclusion. 
Now the above steps of random number assignment and vertex selection are repeated for the vertices left in C, and I is augmented similarly. This incremental augmentation of I ends when C becomes empty. On the average, this algorithm converges after <eqn>O(\log n)</eqn> such augmentation steps. 


3) PARALLEL ALGORITHM: Shared-address-space parallel formulation of Luby's algorithm.

A parallel formulation of Luby's MIS algorithm for a shared-address-space parallel computer is as follows. Let I be an array of size |V|. At the termination of the algorithm, I[i] will store one, if vertex vi is part of the MIS, or zero otherwise. Initially, all the elements in I are set to zero, and during each iteration of Luby's algorithm, some of the entries of that array will be changed to one. Let C be an array of size |V|. During the course of the algorithm, C [i] is one if vertex vi is part of the candidate set, or zero otherwise. Initially, all the elements in C are set to one. Finally, let R be an array of size |V| that stores the random numbers assigned to each vertex.

During each iteration, the set C is logically partitioned among the p processes. Each process generates a random number for its assigned vertices from C. When all the processes finish generating these random numbers, they proceed to determine which vertices can be included in I. In particular, for each vertex assigned to them, they check to see if the random number assigned to it is smaller than the random numbers assigned to all of its adjacent vertices. If it is true, they set the corresponding entry in I to one. Because R is shared and can be accessed by all the processes, determining whether or not a particular vertex can be included in I is quite straightforward.

Array C can also be updated in a straightforward fashion as follows. Each process, as soon as it determines that a particular vertex v will be part of I, will set to zero the entries of C corresponding to its adjacent vertices. Note that even though more than one process may be setting to zero the same entry of C (because it may be adjacent to more than one vertex that was inserted in I), such concurrent writes will not affect the correctness of the results, because the value that gets concurrently written is the same.

The complexity of each iteration of Luby's algorithm is similar to that of the serial algorithm, with the extra cost of the global synchronization after each random number assignment.

  Bibliography: The serial maximal independent set algorithm was developed by Luby [Lub86] and its parallel formulation on shared-address-space architectures was motivated by the algorithm described by Karypis and Kumar [KK99]. 

  ref. M. Luby. A simple parallel algorithm for the maximal independent set problem. SIAM Journal on Computing, 15(4):1036–1053, 1986
  ref. G. Karypis and V. Kumar. Parallel multilevel k-way partitioning for irregular graphs. SIAM Review, 41(2):278–300, 1999.

  Jones and Plassman [JP93] have developed an asynchronous variation of Luby's algorithm that is particularly suited for distributed memory parallel computers. In their algorithm, each vertex is assigned a single random number, and after a communication step, each vertex determines the number of its adjacent vertices that have smaller and greater random numbers. At this point each vertex gets into a loop waiting to receive the color values of its adjacent vertices that have smaller random numbers. Once all these colors have been received, the vertex selects a consistent color, and sends it to all of its adjacent vertices with greater random numbers. The algorithm terminates when all vertices have been colored. Note that besides the initial communication step to determine the number of smaller and greater adjacent vertices, this algorithm proceeds asynchronously.

  ref. M. T. Jones and P. E. Plassmann. A parallel graph coloring heuristic. SIAM Journal on Scientific Computing, 14:654–669, 1993.
</text>

</document>


<!-- Bibliographic notes -->

<document>
<tag>graph-exploration-notes</tag>
<title>Bibliographic notes</title>
 
<!-- Connected components -->

The connected-components algorithm was discovered by Woo and Sahni [WS89]. 
[WS89] J. Woo and S. Sahni. Hypercube computing: Connected components. Journal of Supercomputing, 3:209–234, 1989.


Cormen, Leiserson, and Rivest [CLR90] discusses ways to efficiently implement disjoint-set data structures with ranking and path compression.
[CLR90] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms. MIT Press, McGraw-Hill, New York, NY, 1990.


Several algorithms exist for computing the connected components; many of them are based on the technique of vertex collapsing, similar to Sollin's algorithm for the minimum spanning tree. Most of the parallel formulations of Sollin's algorithm can also find the connected components. 
Hirschberg [Hir76] and Hirschberg, Chandra, and Sarwate [HCS79] developed formulations of the connected-components algorithm based on vertex collapsing. The former has a complexity of Q(log2 n) on a CREW PRAM with n2 processes, and the latter has similar complexity and uses <eqn>n \lceil n/\log n \rceil</eqn> processors. Chin, Lam, and Chen [CLC81] made the vertex collapse algorithm more efficient by reducing the number of processors to <eqn>n \lceil n/\log^2 n \rceil</eqn> for a CREW PRAM, while keeping the run time at Q(log^2 n). Nassimi and Sahni [NS80] used the vertex collapsing technique to develop a formulation for a mesh-connected computer that finds the connected components in time Q(n) by using n^2 processes.

[Hir76] D. S. Hirschberg. Parallel algorithms for the transitive closure and connected component problem. In Proceedings of the 8th Annual ACM Symposium on the Theory of Computing, 55–57, 1976.

[HCS79] D. S. Hirschberg, A. K. Chandra, and D. V. Sarwate. Computing connected components on parallel computers. Communications of the ACM, 22(8):461– 464, August 1979.

[CLC81] F. Y. Chin, J. Lam, and I. Chen. Optimal parallel algorithms for the connected component problem. In Proceedings of the 1981 International Conference on Parallel Processing, 170–175, 1981.

[NS80] D. Nassimi and S. Sahni. Finding connected components and connected ones on a mesh-connected computer. SIAM Journal of Computing, 9(4):744–757, November 1980.




Other parallel graph algorithms...


Atallah and Kosaraju [AK84] proposed a number of algorithms for a mesh-connected parallel computer. The algorithms they considered are for finding the bridges and articulation points of an undirected graph, finding the length of the shortest cycle, finding an MST, finding the cyclic index, and testing if a graph is bipartite. 

  [AK84] M. J. Atallah and S. R. Kosaraju. Graph problems on a mesh-connected processor array. Journal of ACM, 31(3):649–667, July 1984.


Tarjan and Vishkin [TV85] presented algorithms for computing the biconnected components of a graph. Their CRCW PRAM formulation runs in time Q(log n) by using Q(m+n) processors, and their CREW PRAM formulation runs in time Q(log^2 n) by using Q(n^2/log^2 n) processors.

  [TV85] R. E. Tarjan and U. Vishkin. An efficient parallel biconnectivity algorithm. SIAM Journal on Computing, 14(4):862–874, November 1985.


</document>

</document>
