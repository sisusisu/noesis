<?xml version="1.0"  encoding="ISO-8859-1" ?> 
<?xml-stylesheet type="text/xsl" href="../book.xsl"?>


<!-- Poisson random networks -->

<document>
<tag>random-poisson</tag>
<title>Poisson random graphs: Erdos-Renyi's model</title>


<text>
Purely random networks were among the earliest studied networks, despite the fact that most real-world networks are far from random. But, although real networks are not completely random, the class of random networks provides a baseline for comparison with more structured networks <cite>Lewis 2009</cite>.
</text>

<text>
L: Solomonoff and Rappaport were the first to apply the ideas of epidemics to RANDOM networks (Solomonoff, 1951)
[Solomonoff, R. and A. Rapoport, Connectivity of random nets, Bull. Math. Biophys. 13:107-117 (1951).]
</text>

<text>
L--: Random networks provide a starting point for a number of emergent processes, i.e. we start with a random network and then
observe how it evolves through time into some form of nonrandomness... e.g. the random network representing a new market evolves into a structured
network representing a mature market: The personal computer (PC) is a classical example of a mature and structured
market emerging from a chaotic or random new market. In the early 1980s, the PC market consisted of hundreds of companies offering many different kinds of PCs. By the mid-1990s, the early PC market had matured into a structured oligopoly consisting of a small number of dominant market leaders. The market was no longer random. Instead, it consisted of a few highly connected nodes representing the market leaders.
</text>



<document>
<tag>random-poisson-generation</tag>
<title>Random graph generation</title>

<text>
The generation of random networks with no duplicate links, loops, isolated nodes, nor multiple components is more subtle that as it may seem.
</text>

<text>
Generative algorithms attempt to randomize the degree distribution of a network... A “perfectly random” network is one that follows a binomial
degree distribution
</text>

<text>
L: Gilbert showed how to build a random graph by first constructing a complete graph and then deleting randomly selected links until reaching the desired number of links (Gilbert, 1959)... cumbersome algorithm was quickly surpassed by the elegant and widely promoted algorithm
of Erdos and Renyi (Erdos, 1960).

[Gilbert, E. N., Random graphs, Ann. Math. Stat. 30(4):1141-1144 (1959).]
[Erdo¨s, P. and A. Re´nyi, On the evolution of random graphs, Publ. Math. Inst. Hungar. Acad. Sci. 5:17-61 (1960).]
</text>

<note>
<text>
For statistical physicists, a random network is not a particular network, but a statistical ensemble. Such ensembles are defined by their members and their statistical weights, which are proportional to their probabilities. In fact, the classical random graph model is the Gilbert model, <eqn>G_{n,p}</eqn>, where <eqn>n</eqn> is the number of (labeled) nodes and <eqn>p</eqn> is the probability of links. This model represents all possible random graphs with <eqn>n</eqn> nodes and their realization probabilities. Strictly speaking, the notion of randomness is not applicable to individual finite networks <cite>Dogorotsev 2010</cite>. Whereas empirical researchers and computer scientists work with single realizations of networks (or a bunch of them when performing computer simulations), theoretical physicists consider all the members of the corresponding statistical ensemble.
</text>
</note>

<document>
<tag>random-poisson-generation-gilbert</tag>
<title>Gilbert's algorithm</title>

<text>
Gilbert's algorithm  begins with a complete network and then removes links chosen at random, until reaching the desired link density.
</text>

<text>
The basic idea of the Gilbert random network generation procedure is to select links
with probability p from a complete graph with n nodes such that the resulting network
ends up with m = p[n((n-)/2)] links on average. Therefore, a Gilbert
network has density p because a complete network has n((n-1)/2) links.
</text>

<text>
a Gilbert network chooses among the X possible networks with n nodes and m links
</text>

<text>
Algorithm:

Given n and probability p,
1. Initially: generate n nodes
2. Let m = n((n - 1)/2), i.e. the number of links in a complete graph.
for(int i = 0; i &lt; n; i++){
   for(int j = i+1; j &lt; n; j++){
        if (Math.random() &lt; p),
	   add link (i,j)
</text>

<text>
Issues

- Random number from the interval [0,1) to determine whether a link is inserted into the network, testing each of the n((n-1)/2) possible links
and randomly inserts an average of p[n((n-1)/2] links between node pairs. Enumerating them in any given order avoids the risk of incorrectly
adding duplicate links or loops.

- Note that the number of inserted links increases as p increases. Gilbert's method produces a random network with approximately p density. The p density parameter determines the number of links as a byproduct, but only on average, since the number of actual links will vary from network to network given this randomized algorithm. On average, the algorithm will return a network with m = p n (n-1) / 2 links, but the actual number of links in the final network is uncertain (?)...

- Gilbert networks may not be connected. In fact, Gilbert showed that random networks
generated by his procedure contain more than one component with probability n(1-p)^{n-1}
and isolated nodes (with degree equal to zero) with probability 2(1-p)^{n-1}. 
</text>

</document>



<document>
<tag>random-poisson-generation-er</tag>
<title>Erdos-Renyi's algorithm</title>

<text>
Also proposed in 1959, by Paul Erdos and Alfred Renyi, the Erdos-Renyi's algorithm generates random networks by inserting links between randomly chosen node pairs until reaching the desired number of links, avoiding the uncertainty in Gilbert's generation algorithm.

L: A network with n nodes is constructed by inserting a link between randomly selected nodepairs. The process is repeated until m links have been inserted.
</text>

<text>
Algorithm:

Given n and m, 
1. Initially: Generate n nodes
2. Initially, #links (number of links) = 0.
3. Repeat until m = #links have been inserted:
	a. Select random node: tail = random(n)
	b. Select random node: head = random(n)
	c. Avoid loop: while (tail == head) head = random(n)
	d  Avoid duplicate: if (not duplicate) 
		insert new link between tail and head and increment #links.
</text>

<text>
- Unlike Gilbert's algorithm, ER networks are guaranteed to have exactly m links, obey the binomial
degree distribution, and are easy to generate.

- it can leave some nodes isolated (This can occur
because each node pair is selected at random, which means that it is possible a node is
not selected at all.)

- Careful implementation of the ER generation procedure avoids loops and duplicate links, but it does not guarantee a strongly connected network: guarantee that the tail and head nodes of added links are different + repeat the selection process when a duplicate link is chosen (note: efficiency--)
</text>

<text>
L: Erdos–Renyi networks always have a specific number of links, so the density is 2m/(n(n-1)) for a given number of nodes n and links m.
</text>

<text>
L: Gilbert and ER networks have essentially the same properties even though two different sets of microrules produce them.

Both ER and Gilbert procedures produce a random network because the degree distribution asymptotically approaches the Poisson distribution in both cases. Consequently, the entropy and cluster coefficients are comparable when the number of nodes and number of links are equivalent. However, the
Gilbert procedure generates a random network with a certain density, and the ER procedure generates a random network with a certain number of links. i.e., the ER procedure fixes the number of links while the Gilbert does not
</text>

</document>

<document>
<tag>random-poisson-generation-anchored</tag>
<title>Lewis' anchored algorithm</title>

<text>
L:Gilbert and ER procedures produce networks that
may contain isolated nodes, and multiple components... Both Gilbert and ER procedures produce a disconnected network with nonzero probability, so a third generative procedure (anchored ER) is provided that guarantees a connected graph at the expense of some randomness. Anchored random networks are slightly nonrandom, but reduce the occurrence of isolated nodes... The proposed anchored
generative procedure guarantees that all nodes connect to at least one other node, but sacrifices some randomness.
</text>

<text>
L: A slight modification to the ER generative procedure guarantees
that all nodes are connected to at least one other node, by ensuring that every
node has at least one link. This is done by visiting every node at least once—in
round-robin style—and testing the degree of each node. If the degree is zero, the
algorithm attaches the tail of the link to the solitary node; otherwise, it selects a
tail node at random.
</text>

<text>
Algorithm:

Given n and m, construct an anchored random network as follows:
1. Initially: Generate n nodes
2. Initially: m \geq (n/2) given, and #links=0.
3. Repeat until m = #links have been inserted:
	a. Round robin: i ¼ 0,1,2, . . . ,(n21); 0,1,2, . . . .
		b. Select tail: 
			If (degree(i) &gt; 0) 
				tail = random(n), 
			else 
				tail = i.
		c. Select random node: 
			head = (Math.random())n.
		d. Avoid loop: 
			While (tail == head) head = random(n).
		e. Avoid duplicate: 
			If (no duplicate), insert new link between tail and head and increment #links.
</text>

<text>
- The resulting anchored random network must have at least n/2 links, so that every node has at least one link attached to it.

- small bias introduced by the first systematic pass over all nodes in order.
The degree distribution is skewed slightly because all nodes have nonzero
degree when m \geq (n/2). This bias shows up as an entropy that is slightly lower than
that of a purely random network. Recall that the degree distribution obeys a
binomial distribution, and the probability that a node links to nothing is given by B(0,n), which is nonzero.

- While the anchored random network procedure guarantees that all nodes connect
to something,  an anchored random network may still contain separate components
because it is possible for one disjoint subset of nodes to link only to nodes within
the subset, leaving the remaining nodes unreachable from the subset. (note: this is
extremely rare, but the possibility exists)
</text>
</document>

</document>


<!-- Degree distribution -->


<document>
<tag>random-poisson-degree</tag>
<title>Degree distribution</title>

<text>
The degree distributions obtained by by both Gilbert and ER generation algorithms obey a binomial distribution, which can be approximated by a Poisson distribution for large networks (n &gt;&gt; 1), since the binomial distribution is asymptotically equal to the Poisson distribution for
very large m. 

Both distributions represent the probability of a node having degree k, but the Poisson distribution eliminates m (number of links) from the equation, hence it is often preferred.
</text>

<text>
Let \lambda the average degree  from the degree distribution
</text>

<equation>
\lambda = \langle d \rangle = 2m/n
</equation>

<text>
According to the ER generation procedure, each node receives an average of <eqn>\lambda</eqn> connections out of <eqn>m</eqn> links.
Let us focus on a particular node, which will be selected one with probability <eqn>p = (\lambda/m)</eqn>, 
and <eqn>k</eqn> times in a row with probability <eqn>p_k = (\lambda/m)^k</eqn>. That chosen node is not selected at any other time with probability
</text>

<equation>
(1-p)^{m-k} = \left( \frac{1-\lambda}{m} \right) ^ {m-k}
</equation>

<text>
Therefore, the probability that the node is selected exactly the first <eqn>k</eqn> times out of <eqn>m</eqn> is the product of these probabilities:
</text>

<equation>
P_{first}(k) =  \left( \frac{\lambda}{m} \right) ^ {k} \left( \frac{1-\lambda}{m} \right) ^ {m-k}
</equation>

<text>
However, our node can be chosen <eqn>k</eqn> times in many different ways, the number of <eqn>k</eqn>-combinations of <eqn>m</eqn> elements, which is given by the binomial coefficient
</text>

<equation>
{m \choose k} = \frac{m!}{k!(m-k)!}
</equation>

<text>
Therefore, the probability of obtaining exactly <eqn>k</eqn> successes (degree <eqn>k</eqn>) in <eqn>m</eqn> trials (<eqn>m</eqn>links) is given by the binomial distribution: 
</text>

<equation> 
P(k) = B(m,k) = {m \choose k } \left( \frac{\lambda}{m} \right) ^ {k} \left( \frac{1-\lambda}{m} \right) ^ {m-k}
</equation>

<text>
where the success probability in each trial of the binomial distribution is given by <eqn>p=\lambda/m</eqn> and the number of trials is <eqn>n=m</eqn>.
</text>

<text>
The mean of a binomial distribution <eqn>B(n,p)</eqn> with parameters <eqn>n</eqn> and <eqn>p</eqn> is <eqn>np</eqn>, hence the mean number of times a node is choosen will be <eqn>\mu = np = m * (\lambda/m) = \lambda</eqn> as expected. The variance of the binomial distribution is <eqn>\sigma^2=np(1-p)</eqn>, which can be expressed in terms of our initial parameters as <eqn>\sigma^2 = \lambda (1- \lambda/m)</eqn>. When the number of trials is large enough, the skew of the binomial distribution is not too large and an excellent approximation to the binomial distribution is given by the normal distribution <eqn>N(\mu,\sigma^2)</eqn>, which indicates that the binomial distribution exhibits a exponentially-bounded tail.
</text>

<note>
<text>
In random networks, degree distributions decay fast, <eqn>P(k) \sim 1/k!</eqn>. All the moments <eqn>\sum_k k^n P(k)</eqn> of their degree distribution are always finite, hence the mean degree <eqn>\langle k \rangle = \sum_k k P(k)</eqn> is a typical scale for degrees, even on infinite networks. Other kinds of networks, such as scale-free networks, have slowly decaying degree distributions that follow a power-law, <eqn>P(k) \sim k^{-\gamma}</eqn>, are characterized by the lack of a typical node degree. In particular, if the exponent <eqn>\gamma \leq n+1</eqn>, the <eqn>n</eqn>th and higher moments of the distribution diverge (i.e. they are not finite, at least in theory, not so in a particular network instance). In practice, that means that we cannot pick a single node and assume that the rest of the network behaves in a similar way, an assumption often employed to analyze complex networks using a mean-field approximation. 
</text>

<text>
Mean-field theory, from statistical physics, is a method employed to analyze physical systems with multiple bodies. When all the elements (bodies) are considered equivalent, no individual properties can distiguish them from the rest. Their expected value <eqn>\langle f_i \rangle</eqn> for a given property <eqn>f</eqn> is the same. Then, we can often approximate a property of the whole system <eqn>\langle \sum_i f_i \rangle</eqn> by estimating a property for an individual element, <eqn>\sum_i \langle f_i \rangle</eqn>, which greatly simplifies our original problem (provided that we are able to properly capture the effects of the interactions among the elements within the complex system).
</text>
</note>

<text>
However, we will prefer a different approximation. The binomial distribution converges towards the Poisson distribution as the number of trials goes to infinity while its mean <eqn>np = \lambda</eqn> remains fixed. Therefore the Poisson distribution with parameter <eqn>\lambda</eqn> will be used to describe the degree distribution of a random network with average degree <eqn>\lambda</eqn>.
</text>

<text>
Formally, we can see that our approximation is coorrect if we take into account that <eqn>((1-\lambda)/m)^m</eqn> becomes <eqn>e^{-\lambda}</eqn> as <eqn>m</eqn> goes to infinity<!--, since <eqn>\lambda/m</eqn> vanishes more slowly than <eqn>((1-\lambda)/m)^m</eqn> increases-->. This can be proved by expanding <eqn>((1-\lambda)/m)^m</eqn> as a Taylor series around <eqn>x=1</eqn> and observing that the resulting Taylor series expansion is identical to the Taylor series expansion of <eqn>e^{-\lambda}</eqn>.
</text>

<equation> 
\begin{aligned}
P(k) &amp; = {m \choose k } \left( \frac{\lambda}{m} \right) ^ {k} \left( \frac{1-\lambda}{m} \right) ^ {m-k} \\
     &amp; = \frac{m!}{k!(m-k)!} \left( \frac{\lambda}{m} \right) ^ {k} \left( \frac{1-\lambda}{m} \right) ^ {m-k} \\
     &amp; = \frac{m(m-1)...(m-k+1)}{m^k} \left( \frac{\lambda^k}{k!} \right) \left( \frac{1-\lambda}{m} \right) ^ {m-k} \\
     &amp; \rightarrow 1 \left( \frac{\lambda^k}{k!} \right) e^{-\lambda}  =  \frac{\lambda^k e^{-\lambda}}{k!}
\end{aligned}
</equation>

<text>
And this is just a Poisson distribution <eqn>P(\lambda,k)</eqn> with mean and variance <eqn>\lambda</eqn>. Therefore, the degree distribution of a random network is given by 
</text>

<equation> 
P(k) = \lambda^k \frac{e^{-\lambda}}{k!}
</equation>

<text>
where <eqn>\lambda</eqn> is the mean node degree and <eqn>P(k)</eqn> denotes the probability of a node having degree <eqn>k</eqn>. Please, note that the actual number of links, <eqn>m</eqn>, does not appear in this final equation, a fact that simplifies the analytical study of random network.
</text>

<!--
The sum of a Bernoulli process can be thought of as the discrete-time counterpart of a Poisson process (a continuous-time stochastic process that  counts the number of events and the time that these events occur in a given time interval, so that the time between each pair of consecutive events has an exponential distribution with parameter \lambda and each of these inter-arrival times is assumed to be independent of other inter-arrival times), a good model of radioactive decay, telephone calls and requests for a particular document on a web server...

Named after the French mathematician Siméon-Denis Poisson (Pithiviers, Loiret, 21 June 1781 – Sceaux, Hauts-de-Seine, 25 April 1840) and the Dutch-Swiss mathematician Daniel Bernoulli (Groningen, Netherlands, 8 February 1700 – Basel, Switzerland, 17 March 1782)
-->

<text>
++ Figure: @L: Gilbert and ER distributions are similar, but the anchored network distribution is slightly skewed to the left, and has a slightly smaller variance. This bias is a consequence of the anchored ER generation procedure, which purposely connects all nodes to at least one other node, which makes the resulting networks "almost" random.
</text>

</document>




<document>
<tag>random-poisson-entropy</tag>
<title>Entropy</title>

<text>
The entropy of a random network is expected to be high, and it is also expected to drop as the random network becomes denser (since, in the limit, it will approach a complete network).
</text>

<text>
Experimentally, it can be shown that entropy (i.e. randomness) increases quickly as density increases, levels off, and then declines as network density approaches 100%. A fully-random network is only achieved when density is 50% and <eqn>m = n((n-1)/4)</eqn>, whereas entropy will be 0 at both ends of the spectrum: at 0% density and also at 100% density. Therefore, random networks become less random as their density diverges from 50%! <cite>Lewis 2009</cite>.
</text>

<text>
We can derive an empirical expression for the randomness (I mean, entropy) of a random network as proposed by <cite>Lewis 2009</cite>. Let <eqn>\rho</eqn> be the density of the network:
</text>

<equation>
\rho = \frac {2m} {n(n-1)}
</equation>

<text>
Let us now suppose that the entropy of a random network is symmetric around 50% density (just as an approximation). Entropy is maximized at 50% density, whereas entropy declines exponentially at both ends of the density spectrum. Entropy can thus be approximated as
</text>

<equation>
H_{0-50\%}(\rho) \approx M \left( 1 - e^{-R\rho} \right)
</equation>

<text>
and
</text>

<equation>
H_{50-100\%}(\rho) \approx M \left( 1 - e^{-R(1-\rho)} \right)
</equation>

<text>
Combining both expressions, we get
</text>

<equation>
\begin{aligned}
H(\rho) &amp; = \frac{1}{2} H_{0-50\%}(\rho) + \frac{1}{2} H_{50-100\%}(\rho) \\
        &amp; \approx M \left( 1 - \frac{1}{2} e^{-R\rho} - \frac{1}{2} e^{-R(1-\rho)} \right)
\end{aligned}
</equation>

<text>
<cite>Lewis 2009</cite> resorts to a least-squares curve fit to get rough values for <eqn>M</eqn> and <eqn>R</eqn>. Using small random networks, with <eqn>n=100</eqn>, he reports <eqn>M=4</eqn> and <eqn>R=13</eqn> as reasonable values for approximating the entropy of random networks of medium densities.
</text>

<text>
The key result from this section is that the entropy of a random network is a function of its density: entropy increases from zero to a maximum value, and then back to zero again, as the number of links grows. Random networks become less random as their density approaches that of a complete network and, somewhat surprisingly, they are truly random only for medium values of density, but not for low or high densities. 
</text>

</document>


<document>
<tag>random-poisson-diameter</tag>
<title>Network diameter</title>


<text>
- Sparse random networks exhibit the small-world effect: their diameter rapidly shrinks with the addition of a small number of random links. 

Random networks are highly link-efficient because a small increase in number of links has a major impact on the decline of average path length (small-world effect: As density increases, random networks rapidly become "smaller"). Theoretically, the diameter of a random network is log(n)/log(\lambda) where \lambda is the average degree of nodes in the network.
</text>

<text>
Newman, M. E. J., C. Moore, and D. J. Watts, Mean-field solution of the small-world network model, Phys. Rev. Lett. 84(14):3201–3204 (2000).
Newman, M. E. J., Models of the small world: A review, J. Stat. Phys. 101:819–841 (2000).
Newman, M. E. J., Small worlds, the structure of social networks, J. Stat. Phys. 101(3/4) (2000).
Albert, R. and A.-L. Barabasi, Statistical mechanics of complex networks, Rev. Mod. Phys. 74:47–97 (2002)
</text>

<text>
L: Newman (Newman, 2000b; Albert, 2002) derived an approximation for the distance between nodes in a random network with average node degree <eqn>\lambda</eqn>, by assuming that the number of nodes reached along a path from arbitrary node to another node is approximately <eqn>\lambda^D = n</eqn>. Newman argued that <eqn>\lambda</eqn> nodes can be reached in 1 hop, <eqn>\lambda^2</eqn> in 2 hops, <eqn>\lambda^3</eqn> in 3 hops, and so on, until n nodes are reached in <eqn>\lambda^D</eqn> hops. Solving for D, we obtain the network diameter
</text>

<equation>
D = \frac{\log n}{\log \lambda}
</equation>

<text>
However, this computation of the network diameter assumes that the network is an acyclic tree with each node connecting to exactly <eqn>\lambda</eqn> other nodes. A slightly more accurate approximation can be obtained by observing that a path from given node to the other nodes in the same connected subgraphs reaches <eqn>\lambda</eqn> nodes in 1 hop and <eqn>\lambda(\lambda-1)</eqn> nodes in 2 hops, since theneighbors of neighbors only have <eqn>\lambda-1</eqn> outgoing links that do not return to the visited node. Subsequent link traversals reach <eqn>\lambda(\lambda-1)^{d-1}</eqn> nodes at distance <eqn>d</eqn>. If the entire network belonged to a giant, strongly connected subgraph, then all <eqn>n</eqn> nodes would be reached in <eqn>D</eqn> hops:
</text>

<equation>
n = \lambda(\lambda-1)^{D-1}
</equation>

<text>
Solving for <eqn>D</eqn>, we obtain a better approximation for the network diameter:
</text>

<equation>
D = \frac{ \log n - \log \lambda }{ \log(\lambda-1) + 1 }
</equation>


</document>


<document>
<tag>random-poisson-properties</tag>
<title>Average path length</title>

<text>
L: The average path length of a random network should decrease as the number of links
increases because the number of paths between node pairs proliferates—thus providing
more opportunity for shorter alternative paths. 

L: In fact, average path length should asymptotically
reach 1 hop as density reaches 100% because a fully connected network connects
every node to every other node.

Figure: When the average path lengths of ER and Gilbert random networks are plotted on a log–log scale, average path length dramatically
falls off as density increases up to 100%. 
</text>


<text>
The average path length of a uniformly random network is proportional to its diameter, and so <eqn>D</eqn> could be used as a relatively good approximation for the average path length, as 
</text>

<equation>
L \approx D - 1 
</equation>

<text>
Link efficiency... Random networks are highly efficient users of links because of the small-world effect.
Because a small amount of randomness in any network injects a major drop in average path length, randomness results in a large jump in link efficiency! </text>

</document>



<document>
<tag>random-poisson-clustering</tag>
<title>Clustering coefficients</title>


<text>
- Cluster coefficients are expected to be small in comparison with more structured networks.

As the average path length decreases because of an increase in the density of links in a
random network, the cluster coefficient does the opposite: it should increase because more links means that
triangular subgraphs are more likely to form (recall that clusters
are created by linking adjacent neighbors to form triangular subgraphs)
</text>


<text>
When density equals 100%, the network is no longer random—it is complete. In a complete network, every node is connected to every other node, so the cluster coefficient of every node is 1.0. Alternatively, as a network becomes sparse, its cluster coefficient decreases—
ultimately to zero.
</text>

<text>
Watts and Strogatz (Watts, 1998) derived a theoretical estimate of random network cluster coefficient, as follows
</text>

<equation>
CC = \frac { \lambda } { n } = \rho
</equation>

<text>
that is, clustering is directly proportional to the number of (random) links added to the network. In fact, the clustering coefficient of a random network increases linearly with its density (both have equal values). 
</text>

<text>
By definition, cluster coefficient of average node v with mean degree <eqn>\lambda</eqn> is:
</text>

<equation>
CC(v) = \frac { 2c } { \lambda (\lambda -1 ) }
</equation>

<text>
where c is the number of links among adjacent nodes. Using a mean-field approach to estimation, 
the number of links among adjacent nodes (forming triangular subgraphs) is <eqn>\lambda</eqn> things taken two at a time:
</text>

<equation>
{\lambda \choose 2} = \frac { \lambda! } { 2 (\lambda - 2 )! } = \frac { \lambda (\lambda -1) } { 2 }
</equation>

<text>
for fully-connected networks, but just the following for sparse networks:
</text>

<equation>
\rho {\lambda \choose 2} = \rho \frac { \lambda (\lambda -1) } { 2 }
</equation>

<text>
But, since the proportion of links present in a cluster equals the network's overall density, the mean-field approximation for <eqn>c</eqn> is also
</text>

<equation>
c = \rho {\lambda \choose 2} = \rho \frac { \lambda (\lambda -1) } { 2 } = \frac { \rho  \lambda (\lambda - 1 ) } { 2 }
</equation>

<text>
Substituting in the expression for the clustering coefficient, we obtain:
</text>

<equation>
CC(v) = \frac { 2c } { \lambda (\lambda -1 ) } =  \rho = \frac { \lambda } { n }
</equation>


</document>


<document>
<tag>random-poisson-centrality</tag>
<title>Centrality measures</title>

<text>
(X) Motivation: Social network analysis is often interested in the power of actors (nodes) in a social network. But the definition of “power” often varies from one application to another. It might be related to how many links connect an actor to others (hub analysis), how far away an actor is from all other actors (radius), or the intermediary position of an actor within the network (betweenness)? Sometimes, as in Milgram's experiment, diameter and radius are of interest, radius might be more appropriate for epidemiology, and betweenness may be more appropriate for analyzing network dynamics.
</text>

<!-- Radius -->

<text>
Diameter and radius decrease as network density increases, as expected.

Surprisingly, radius can be larger than average path length.
since the radius of the central node is not the minimum of shortest paths, but rather the minimum of longest paths!
(the number of hops from the central node to the most remote node)

The average path, on the contrary, is an
average over short and long paths. Therefore, it is entirely possible for the average
path length to be smaller than the radius of the central node. The radius of a
network is very large if only one pair of nodes is separated by a long distance.
</text>

<!-- Betweenness -->

<text>
A node is considered important to other nodes if many shortest paths connecting the other nodes run through the close node.

Recall that betweenness of node w is defined as the number of direct paths from every other node u to every other node v running through node w.

... a connectivity metric.
</text>

<text>
Diameter and radius plummet with increase in density, but betweenness does not!

This counterintuitive result is shown to be a consequence of two forces acting simultaneously
on the structure of direct paths in a random network

1) increasing the number of links also increases the number of paths through a typical node

2) increasing density also decreases the average path length, which decreases betweenness
</text>


<text>
The betweenness of an average node increases with density, up to a maximum value: due to the increase in number of links because the number of paths increases

And then it decreases with density (eventually,
the increase in links produces shorter paths because of the small-world effect. These
shorter paths bypass the (larger number of) alternative and longer paths. Short
paths short-circuit the longer ones, which tend to decrease the number of paths
running through a typical node. At some density point, the number of paths
running through a typical node starts to decline)

In other words, betweenness increases with an increase in the number of links,
but is soon overwhelmed by the rapid drop in length of shortest paths, due to the
small-world effect.
</text>

<text>
two factors: average path length and number of shortestt paths.
</text>

<text>
We estimate the number of paths passing through a typical node using a mean-field approach. Node v has an average of \lambda links attached to it.
Each link connects v to approximately \lambda^D other nodes, where D is the diameter of the network. Therefore, the number of paths through node v should be proportional to \lambda^D. But we know that increases in density lead to shortcuts around node v. The number of paths through v decreases with density, according to (1-\rho). As an approximation, we get betweenness(random) \sim (1 - \rho) \lambda^D, which supports the conjecture
that average betweenness is a function of sparseness (1-density) and average path length.
</text>

<text>

DRAFT...

Apparently, there is a somewhat linear relationship between betweenness and path length:
...
betweenness(random) \sim L(random) = D 

D = \frac{\log n}{\log \lambda}
D = \frac{ \log n - \log \lambda }{ \log(\lambda-1) + 1 } 

\lambda = \rho n
...
betweenness(random) = \frac { \log n } { \log (n\rho) } = \frac {1}{\rho}
...

In addition, the number of paths running through the closest node varies according to #paths(intermediary)  \sim \ell
</text>


<text>
- Betweenness/closeness properties are small in comparison with other structured networks.
</text>


</document>




</document>
