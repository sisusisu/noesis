<?xml version="1.0"  encoding="ISO-8859-1" ?> 
<!DOCTYPE sys-ents [ <!ENTITY bibliography  SYSTEM "bibliography.xml"> ]>
<?xml-stylesheet type="text/xsl" href="../book.xsl"?>


<!-- Poisson random networks -->

<document>
&bibliography;
<tag>random-poisson</tag>
<title>Poisson random graphs</title>


<text>
Purely random networks were among the earliest studied networks, despite the fact that most real-world networks are far from random. But, although real networks are not completely random, the class of random networks provides a baseline for comparison with more structured networks <cite>Lewis 2009</cite>.
</text>

<text>
L: Solomonoff and Rappaport were the first to apply the ideas of epidemics to RANDOM networks and they proposed the Gilbert model of random graphs, as it is known since it was rediscovered by E. N. Gilbert in 1959 
[Ray L. Solomonoff and Anatol Rapoport, Connectivity of random nets, Bulletin of Mathematical Biophysics 13:107-117 (1951).]
[Ray L. Solomonoff and Anatol Rapoport, An exact method for the computation of the connectivity of random nets, Bulletin of Mathematical Biophysics 14:153 (1952).]
</text>

<text>
L--: Random networks provide a starting point for a number of emergent processes, i.e. we start with a random network and then
observe how it evolves through time into some form of nonrandomness... e.g. the random network representing a new market evolves into a structured
network representing a mature market: The personal computer (PC) is a classical example of a mature and structured
market emerging from a chaotic or random new market. In the early 1980s, the PC market consisted of hundreds of companies offering many different kinds of PCs. By the mid-1990s, the early PC market had matured into a structured oligopoly consisting of a small number of dominant market leaders. The market was no longer random. Instead, it consisted of a few highly connected nodes representing the market leaders.
</text>

<text>
The average distance and clustering coefficient only depend on the number of nodes and edges in the network.
+ This suggests that general models based only on the number of nodes and edges in the network could be successful in describing the properties of an "expected" (characteristic) network.
+ Random network: distributes the edges randomly among nodes.
+ Probabilistic interpretation: There exists a set (ensemble) of networks with given number of nodes and edges. Select a random member of this set
+ Expected properties of this network studied by random graph theory (when <eqn>n \rightarrow \infty</eqn>).
</text>

<document>
<tag>random-poisson-generation</tag>
<title>Random graph generation</title>

<text>
The generation of random networks with no duplicate links, loops, isolated nodes, nor multiple components is more subtle that as it may seem.
</text>

<text>
Generative algorithms attempt to randomize the degree distribution of a network... A “perfectly random” network is one that follows a binomial
degree distribution
</text>

<text>
L: Gilbert showed how to build a random graph by first constructing a complete graph and then deleting randomly selected links until reaching the desired number of links (Gilbert, 1959)... cumbersome algorithm was quickly surpassed by the elegant and widely promoted algorithm
of Erdos and Renyi <cite>Erdos and Renyi 1960</cite>.

[Gilbert, E. N., Random graphs, Ann. Math. Stat. 30(4):1141-1144 (1959).]
</text>

<note>
<text>
For statistical physicists, a random network is not a particular network, but a statistical ensemble. Such ensembles are defined by their members and their statistical weights, which are proportional to their probabilities. In fact, the classical random graph model is the Gilbert model, <eqn>G_{n,p}</eqn>, where <eqn>n</eqn> is the number of (labeled) nodes and <eqn>p</eqn> is the probability of links. This model represents all possible random graphs with <eqn>n</eqn> nodes and their realization probabilities. Strictly speaking, the notion of randomness is not applicable to individual finite networks <cite>Dorogotsev 2010</cite>. Whereas empirical researchers and computer scientists work with single realizations of networks (or a bunch of them when performing computer simulations), theoretical physicists consider all the members of the corresponding statistical ensemble.
</text>
</note>

<document>
<tag>random-poisson-generation-gilbert</tag>
<title>Gilbert's algorithm</title>

<text>
Gilbert's algorithm  begins with a complete network and then removes links chosen at random, until reaching the desired link density.
</text>

<text>
The basic idea of the Gilbert random network generation procedure is to select links
with probability p from a complete graph with n nodes such that the resulting network
ends up with m = p[n((n-)/2)] links on average. Therefore, a Gilbert
network has density p because a complete network has n((n-1)/2) links.
</text>

<text>
a Gilbert network chooses among the X possible networks with n nodes and m links
</text>

<text>
Algorithm:

Given n and probability p,
1. Initially: generate n nodes
2. Let m = n((n - 1)/2), i.e. the number of links in a complete graph.
for(int i = 0; i &lt; n; i++){
   for(int j = i+1; j &lt; n; j++){
        if (Math.random() &lt; p),
	   add link (i,j)
</text>

<text>
Issues

- Random number from the interval [0,1) to determine whether a link is inserted into the network, testing each of the n((n-1)/2) possible links
and randomly inserts an average of p[n((n-1)/2] links between node pairs. Enumerating them in any given order avoids the risk of incorrectly
adding duplicate links or loops.

- Note that the number of inserted links increases as p increases. Gilbert's method produces a random network with approximately p density. The p density parameter determines the number of links as a byproduct, but only on average, since the number of actual links will vary from network to network given this randomized algorithm. On average, the algorithm will return a network with a expected number of links equal to m = p n (n-1) / 2, but the actual number of links in the final network might differ, since it is generated by a randomized algorithm.

- Gilbert networks may not be connected. In fact, Gilbert showed that random networks
generated by his procedure contain more than one component with probability n(1-p)^{n-1}
and isolated nodes (with degree equal to zero) with probability 2(1-p)^{n-1}. 
</text>

</document>



<document>
<tag>random-poisson-generation-er</tag>
<title>Erdos-Renyi's algorithm</title>

<text>
Also proposed in 1959, by Paul Erdos and Alfred Renyi, the Erdos-Renyi's algorithm generates random networks by inserting links between randomly chosen node pairs until reaching the desired number of links, avoiding the uncertainty in Gilbert's generation algorithm.

L: A network with n nodes is constructed by inserting a link between randomly selected nodepairs. The process is repeated until m links have been inserted.
</text>

<text>
Algorithm:

Given n and m, 
1. Initially: Generate n nodes
2. Initially, #links (number of links) = 0.
3. Repeat until m = #links have been inserted:
	a. Select random node: tail = random(n)
	b. Select random node: head = random(n)
	c. Avoid loop: while (tail == head) head = random(n)
	d  Avoid duplicate: if (not duplicate) 
		insert new link between tail and head and increment #links.
</text>

<text>
- Unlike Gilbert's algorithm, ER networks are guaranteed to have exactly m links, obey the binomial
degree distribution, and are easy to generate.

- it can leave some nodes isolated (This can occur
because each node pair is selected at random, which means that it is possible a node is
not selected at all.)

- Careful implementation of the ER generation procedure avoids loops and duplicate links, but it does not guarantee a strongly connected network: guarantee that the tail and head nodes of added links are different + repeat the selection process when a duplicate link is chosen (note: efficiency--)
</text>

<text>
L: Erdos–Renyi networks always have a specific number of links, so the density is 2m/(n(n-1)) for a given number of nodes n and links m.
</text>

<text>
L: Gilbert and ER networks have essentially the same properties even though two different sets of microrules produce them.

Both ER and Gilbert procedures produce a random network because the degree distribution asymptotically approaches the Poisson distribution in both cases. Consequently, the entropy and cluster coefficients are comparable when the number of nodes and number of links are equivalent. However, the
Gilbert procedure generates a random network with a certain density, and the ER procedure generates a random network with a certain number of links. i.e., the ER procedure fixes the number of links while the Gilbert does not
</text>

</document>

<document>
<tag>random-poisson-generation-anchored</tag>
<title>Lewis' anchored algorithm</title>

<text>
L:Gilbert and ER procedures produce networks that
may contain isolated nodes, and multiple components... Both Gilbert and ER procedures produce a disconnected network with nonzero probability, so a third generative procedure (anchored ER) is provided that guarantees a connected graph at the expense of some randomness. Anchored random networks are slightly nonrandom, but reduce the occurrence of isolated nodes... The proposed anchored
generative procedure guarantees that all nodes connect to at least one other node, but sacrifices some randomness.
</text>

<text>
L: A slight modification to the ER generative procedure guarantees
that all nodes are connected to at least one other node, by ensuring that every
node has at least one link. This is done by visiting every node at least once—in
round-robin style—and testing the degree of each node. If the degree is zero, the
algorithm attaches the tail of the link to the solitary node; otherwise, it selects a
tail node at random.
</text>

<text>
Algorithm:

Given n and m, construct an anchored random network as follows:
1. Initially: Generate n nodes
2. Initially: m \geq (n/2) given, and #links=0.
3. Repeat until m = #links have been inserted:
	a. Round robin: i = 0,1,2, . . . ,(n-1); 0,1,2, . . . .
		b. Select tail: 
			If (degree(i) &gt; 0) 
				tail = random(n), 
			else 
				tail = i.
		c. Select random node: 
			head = (Math.random())n.
		d. Avoid loop: 
			While (tail == head) head = random(n).
		e. Avoid duplicate: 
			If (no duplicate), insert new link between tail and head and increment #links.
</text>

<text>
- The resulting anchored random network must have at least n/2 links, so that every node has at least one link attached to it.

- small bias introduced by the first systematic pass over all nodes in order.
The degree distribution is skewed slightly because all nodes have nonzero
degree when m \geq (n/2). This bias shows up as an entropy that is slightly lower than
that of a purely random network. Recall that the degree distribution obeys a
binomial distribution, and the probability that a node links to nothing is given by B(0,n), which is nonzero.

- While the anchored random network procedure guarantees that all nodes connect
to something,  an anchored random network may still contain separate components
because it is possible for one disjoint subset of nodes to link only to nodes within
the subset, leaving the remaining nodes unreachable from the subset. (note: this is
extremely rare, but the possibility exists)
</text>
</document>

</document>



<document>
<tag>random-poisson-properties</tag>
<title>Structural properties of random graphs</title>

<!-- Degree distribution -->


<document>
<tag>random-poisson-degree</tag>
<title>Degree distribution</title>


<text>
The degree distribution in random graphs completely describes their architecture since links are independent and even connected nodes are statistically independent. Because of this, random graphs are <term>uncorrelated network</term>s and the absence of correlation between node degrees allows the factorization of joint distributions (i.e. the probability that a node has degree x and another one has degree y is just P(x,y)=p(x)p(y)).
</text>

<text>
The degree distributions obtained by by both Gilbert and ER generation algorithms obey a binomial distribution, which can be approximated by a Poisson distribution for large networks (n &gt;&gt; 1), since the binomial distribution is asymptotically equal to the Poisson distribution for
very large m. 

Both distributions represent the probability of a node having degree k, but the Poisson distribution eliminates m (number of links) from the equation, hence it is often preferred.
</text>

<text>
Let \lambda the average degree  from the degree distribution
</text>

<equation>
\lambda = \langle d \rangle = 2m/n \approx p (n-1)
</equation>


<text>
According to the ER generation procedure, each node receives an average of <eqn>\lambda</eqn> connections out of <eqn>m</eqn> links.
Let us focus on a particular node, which will be selected one with probability <eqn>p = (\lambda/m)</eqn>, 
and <eqn>k</eqn> times in a row with probability <eqn>p_k = (\lambda/m)^k</eqn>. That chosen node is not selected at any other time with probability
</text>

<equation>
(1-p)^{m-k} = \left( \frac{1-\lambda}{m} \right) ^ {m-k}
</equation>

<text>
Therefore, the probability that the node is selected exactly the first <eqn>k</eqn> times out of <eqn>m</eqn> is the product of these probabilities:
</text>

<equation>
P_{first}(k) =  \left( \frac{\lambda}{m} \right) ^ {k} \left( \frac{1-\lambda}{m} \right) ^ {m-k}
</equation>

<text>
However, our node can be chosen <eqn>k</eqn> times in many different ways, the number of <eqn>k</eqn>-combinations of <eqn>m</eqn> elements, which is given by the binomial coefficient
</text>

<equation>
{m \choose k} = \frac{m!}{k!(m-k)!}
</equation>

<text>
Therefore, the probability of obtaining exactly <eqn>k</eqn> successes (degree <eqn>k</eqn>) in <eqn>m</eqn> trials (<eqn>m</eqn>links) is given by the binomial distribution: 
</text>

<equation> 
P(k) = B(m,k) = {m \choose k } \left( \frac{\lambda}{m} \right) ^ {k} \left( \frac{1-\lambda}{m} \right) ^ {m-k}
</equation>

<text>
where the success probability in each trial of the binomial distribution is given by <eqn>p=\lambda/m</eqn> and the number of trials is <eqn>n=m</eqn>.
</text>

<text>
The mean of a binomial distribution <eqn>B(n,p)</eqn> with parameters <eqn>n</eqn> and <eqn>p</eqn> is <eqn>np</eqn>, hence the mean number of times a node is choosen will be <eqn>\mu = np = m * (\lambda/m) = \lambda</eqn> as expected. The variance of the binomial distribution is <eqn>\sigma^2=np(1-p)</eqn>, which can be expressed in terms of our initial parameters as <eqn>\sigma^2 = \lambda (1- \lambda/m)</eqn>. When the number of trials is large enough, the skew of the binomial distribution is not too large and an excellent approximation to the binomial distribution is given by the normal distribution <eqn>N(\mu,\sigma^2)</eqn>, which indicates that the binomial distribution exhibits a exponentially-bounded tail.

i.e. Most of the nodes have approximately the same degree and the probability of very highly connected nodes is exponentially small.
</text>

<note>
<text>
In random networks, degree distributions decay fast, <eqn>P(k) \sim 1/k!</eqn>. All the moments <eqn>\sum_k k^n P(k)</eqn> of their degree distribution are always finite, hence the mean degree <eqn>\langle k \rangle = \sum_k k P(k)</eqn> is a typical scale for degrees, even on infinite networks. Other kinds of networks, such as scale-free networks, have slowly decaying degree distributions that follow a power-law, <eqn>P(k) \sim k^{-\gamma}</eqn>, are characterized by the lack of a typical node degree. In particular, if the exponent <eqn>\gamma \leq n+1</eqn>, the <eqn>n</eqn>th and higher moments of the distribution diverge (i.e. they are not finite, at least in theory, not so in a particular network instance). In practice, that means that we cannot pick a single node and assume that the rest of the network behaves in a similar way, an assumption often employed to analyze complex networks using a mean-field approximation. 
</text>

<text>
Mean-field theory, from statistical physics, is a method employed to analyze physical systems with interacting agents. In this approximate approach, instead of solving a full set of equations for all the interacting agents, a single agent is considered (provided that we are able to properly capture the effects of the interactions within the complex system on the selected agent). Obviously, this approach is only suitable for sufficiently homogeneous systems (heterogeneity might not allow us to employ a valid mean-field theory approximation). For example, when all the elements in a complex system are considered to be equivalent, no individual properties can distiguish them from the rest. Their expected value <eqn>\langle f_i \rangle</eqn> for any property <eqn>f</eqn> will be the same. Then, we can often approximate a property of the whole system <eqn>\langle \sum_i f_i \rangle</eqn> by estimating the value of the property for individual elements, <eqn>\sum_i \langle f_i \rangle</eqn>, which greatly simplifies our problem.
</text>
</note>

<text>
However, we will prefer a different approximation. The binomial distribution converges towards the Poisson distribution as the number of trials goes to infinity while its mean <eqn>np = \lambda</eqn> remains fixed. Therefore the Poisson distribution with parameter <eqn>\lambda</eqn> will be used to describe the degree distribution of a random network with average degree <eqn>\lambda</eqn>.
</text>

<text>
Formally, we can see that our approximation is coorrect if we take into account that <eqn>((1-\lambda)/m)^m</eqn> becomes <eqn>e^{-\lambda}</eqn> as <eqn>m</eqn> goes to infinity<!--, since <eqn>\lambda/m</eqn> vanishes more slowly than <eqn>((1-\lambda)/m)^m</eqn> increases-->. This can be proved by expanding <eqn>((1-\lambda)/m)^m</eqn> as a Taylor series around <eqn>x=1</eqn> and observing that the resulting Taylor series expansion is identical to the Taylor series expansion of <eqn>e^{-\lambda}</eqn>.
</text>

<equation> 
\begin{aligned}
P(k) &amp; = {m \choose k } \left( \frac{\lambda}{m} \right) ^ {k} \left( \frac{1-\lambda}{m} \right) ^ {m-k} \\
     &amp; = \frac{m!}{k!(m-k)!} \left( \frac{\lambda}{m} \right) ^ {k} \left( \frac{1-\lambda}{m} \right) ^ {m-k} \\
     &amp; = \frac{m(m-1)...(m-k+1)}{m^k} \left( \frac{\lambda^k}{k!} \right) \left( \frac{1-\lambda}{m} \right) ^ {m-k} \\
     &amp; \rightarrow 1 \left( \frac{\lambda^k}{k!} \right) e^{-\lambda}  =  \frac{\lambda^k e^{-\lambda}}{k!}
\end{aligned}
</equation>

<text>
And this is just a Poisson distribution <eqn>P(\lambda,k)</eqn> with mean and variance <eqn>\lambda</eqn>. Therefore, the degree distribution of a random network is given by 
</text>

<equation> 
P(k) = \lambda^k \frac{e^{-\lambda}}{k!}
</equation>

<text>
where <eqn>\lambda</eqn> is the mean node degree and <eqn>P(k)</eqn> denotes the probability of a node having degree <eqn>k</eqn>. Please, note that the actual number of links, <eqn>m</eqn>, does not appear in this final equation, a fact that simplifies the analytical study of random network.
</text>

<!--
The sum of a Bernoulli process can be thought of as the discrete-time counterpart of a Poisson process (a continuous-time stochastic process that  counts the number of events and the time that these events occur in a given time interval, so that the time between each pair of consecutive events has an exponential distribution with parameter \lambda and each of these inter-arrival times is assumed to be independent of other inter-arrival times), a good model of radioactive decay, telephone calls and requests for a particular document on a web server...

Named after the French mathematician Siméon-Denis Poisson (Pithiviers, Loiret, 21 June 1781 – Sceaux, Hauts-de-Seine, 25 April 1840) and the Dutch-Swiss mathematician Daniel Bernoulli (Groningen, Netherlands, 8 February 1700 – Basel, Switzerland, 17 March 1782)
-->

<text>
++ Figure: @L: Gilbert and ER distributions are similar, but the anchored network distribution is slightly skewed to the left, and has a slightly smaller variance. This bias is a consequence of the anchored ER generation procedure, which purposely connects all nodes to at least one other node, which makes the resulting networks "almost" random.
</text>



<document>
<tag>random-poisson-neighbor</tag>
<title>Neighbor degree distribution</title>

<text>
Let the number of nodes with degree <eqn>d</eqn> be <eqn>n(d)</eqn>. Then <eqn>n = \sum_d n(d)</eqn> and the total degree of the graph is <eqn>\sum_d d n(d) = n \lambda</eqn>, since <eqn>n(d)</eqn> nodes of degree <eqn>d</eqn> connect to <eqn>d n(d)</eqn> nodes. 
</text>

<text>
Starting from any node in the network, the frequency with which one of its neighbors chosen at random will have degree <eqn>d</eqn> is <eqn>d n(d) / n \lambda</eqn>. In terms of probabilities, the probability that a randomly-chosen neighbor has degree <eqn>d</eqn> equals <eqn>d p(d) / \lambda</eqn>, where <eqn>\lambda = \langle d \rangle</eqn>. 
</text>

<text>
In other words, the degree distribution of the neighbors is different from the degree distribution of the nodes in a random network. The average degree of a neighbor will be <eqn>\langle d^2 \rangle / \langle d \rangle</eqn>, which is greater than the network average degree <eqn>\langle d \rangle</eqn>.
</text>

<text>
The above result indicates that, even on random networks, your friends will tend to have more friends than you.
</text>

</document>

<document>
<tag>random-poisson-entropy</tag>
<title>Entropy</title>

<text>
The entropy of a random network is expected to be high, and it is also expected to drop as the random network becomes denser (since, in the limit, it will approach a complete network).
</text>

<text>
Experimentally, it can be shown that entropy (i.e. randomness) increases quickly as density increases, levels off, and then declines as network density approaches 100%. A fully-random network is only achieved when density is 50% and <eqn>m = n((n-1)/4)</eqn>, whereas entropy will be 0 at both ends of the spectrum: at 0% density and also at 100% density. Therefore, random networks become less random as their density diverges from 50%! <cite>Lewis 2009</cite>.
</text>

<text>
We can derive an empirical expression for the randomness (I mean, entropy) of a random network as proposed by <cite>Lewis 2009</cite>. Let <eqn>\rho</eqn> be the density of the network:
</text>

<equation>
\rho = \frac {2m} {n(n-1)}
</equation>

<text>
Let us now suppose that the entropy of a random network is symmetric around 50% density (just as an approximation). Entropy is maximized at 50% density, whereas entropy declines exponentially at both ends of the density spectrum. Entropy can thus be approximated as
</text>

<equation>
H_{0-50\%}(\rho) \approx M \left( 1 - e^{-R\rho} \right)
</equation>

<text>
and
</text>

<equation>
H_{50-100\%}(\rho) \approx M \left( 1 - e^{-R(1-\rho)} \right)
</equation>

<text>
Combining both expressions, we get
</text>

<equation>
\begin{aligned}
H(\rho) &amp; = \frac{1}{2} H_{0-50\%}(\rho) + \frac{1}{2} H_{50-100\%}(\rho) \\
        &amp; \approx M \left( 1 - \frac{1}{2} e^{-R\rho} - \frac{1}{2} e^{-R(1-\rho)} \right)
\end{aligned}
</equation>

<text>
<cite>Lewis 2009</cite> resorts to a least-squares curve fit to get rough values for <eqn>M</eqn> and <eqn>R</eqn>. Using small random networks, with <eqn>n=100</eqn>, he reports <eqn>M=4</eqn> and <eqn>R=13</eqn> as reasonable values for approximating the entropy of random networks of medium densities.
</text>

<text>
The key result from this section is that the entropy of a random network is a function of its density: entropy increases from zero to a maximum value, and then back to zero again, as the number of links grows. Random networks become less random as their density approaches that of a complete network and, somewhat surprisingly, they are truly random only for medium values of density, but not for low or high densities. 
</text>

</document>


</document>



<document>
<tag>random-poisson-diameter</tag>
<title>Network diameter</title>


<text>
- Sparse random networks exhibit the small-world effect: their diameter rapidly shrinks with the addition of a small number of random links. 

Random networks are highly link-efficient because a small increase in number of links has a major impact on the decline of average path length (small-world effect: As density increases, random networks rapidly become "smaller"). Theoretically, the diameter of a random network is log(n)/log(\lambda) where \lambda is the average degree of nodes in the network.
</text>

<text>
Newman, M. E. J., C. Moore, and D. J. Watts, Mean-field solution of the small-world network model, Phys. Rev. Lett. 84(14):3201–3204 (2000).
Newman, M. E. J., Models of the small world: A review, J. Stat. Phys. 101:819–841 (2000).
Newman, M. E. J., Small worlds, the structure of social networks, J. Stat. Phys. 101(3/4) (2000).
Albert, R. and A.-L. Barabasi, Statistical mechanics of complex networks, Rev. Mod. Phys. 74:47–97 (2002)
</text>

<text>
[Chung and Lu, Adv. Appl. Math 26, 257 (2001)] derived an approximation for the distance between nodes in a random network with average node degree <eqn>\lambda</eqn>, by assuming that the number of nodes reached along a path from arbitrary node to another node is approximately <eqn>\lambda^D = n</eqn>. Since <eqn>\lambda</eqn> nodes can be reached in 1 hop, <eqn>\lambda^2</eqn> in 2 hops, <eqn>\lambda^3</eqn> in 3 hops, and so on, until n nodes are reached in <eqn>\lambda^D</eqn> hops.
</text>


<text>
Solving for <eqn>D</eqn>, we obtain the network diameter
</text>

<equation>
D = \frac{\log n}{\log \lambda}
</equation>

<text>
However, this computation of the network diameter assumes that the network is an acyclic tree with each node connecting to exactly <eqn>\lambda</eqn> other nodes. A slightly more accurate approximation can be obtained by observing that a path from given node to the other nodes in the same connected subgraphs reaches <eqn>\lambda</eqn> nodes in 1 hop and <eqn>\lambda(\lambda-1)</eqn> nodes in 2 hops, since theneighbors of neighbors only have <eqn>\lambda-1</eqn> outgoing links that do not return to the visited node. Subsequent link traversals reach <eqn>\lambda(\lambda-1)^{d-1}</eqn> nodes at distance <eqn>d</eqn>. If the entire network belonged to a giant, strongly connected subgraph, then all <eqn>n</eqn> nodes would be reached in <eqn>D</eqn> hops:
</text>

<equation>
n = \lambda(\lambda-1)^{D-1}
</equation>

<text>
Solving for <eqn>D</eqn>, we obtain a better approximation for the network diameter:
</text>

<equation>
D = \frac{ \log n - \log \lambda }{ \log(\lambda-1) + 1 }
</equation>


</document>


<document>
<tag>random-poisson-path-length</tag>
<title>Average path length</title>


<text>
Random graphs tend to have a tree-like topology with almost constant node degrees
</text>

<text>
L: The average path length of a random network should decrease as the number of links
increases because the number of paths between node pairs proliferates—thus providing
more opportunity for shorter alternative paths. 

L: In fact, average path length should asymptotically
reach 1 hop as density reaches 100% because a fully connected network connects
every node to every other node.

Figure: When the average path lengths of ER and Gilbert random networks are plotted on a log–log scale, average path length dramatically
falls off as density increases up to 100%. 
</text>


<text>
The average path length of a uniformly random network is proportional to its diameter, and so <eqn>D</eqn> could be used as a relatively good approximation for the average path length, as 
</text>

<text>
Since there is a small number of cycles in a random network (more on that below, when we discuss their clustering coefficients), we can make use of the local tree-like structure of random graphs
</text>

<text>
- nr. of first neighbors: <eqn>n_1 \approx \lambda</eqn>
- nr. of second neighbors: <eqn>n_2 \approx \lambda^2</eqn>
- ... at maximum distance:
</text>

<equation>>
1 + \sum_{i=1}^{D} \lambda^i = n
</equation>

<text>
If we ignore the presence of cycles, we can assume that <eqn>n \approx \lambda^D</eqn> for large random networks and also that <eqn>n \approx \lambda^L</eqn>, being <eqn>L</eqn> the average path length in the network. Therefore,
</text>

<equation>
L \approx D 
</equation>

<text>
or, equivalently,
</text>

<equation>
L \approx \frac{ \log n }{ \log \lambda } 
</equation>

<text>
This formal result is valid for all uncorrelated networks and can be achieved using a different route. The average branching of a node is the average of the number of connections at the end of a randomly chosen link, minus one (the link itself). Since that average is given by the neighbor degree distribution, we know that the average branching will be <eqn>\langle d^2 \rangle / \langle d \rangle - 1</eqn>. But, for Poisson distributions <eqn>\langle d^2 \rangle - \langle d \rangle = \langle d^2 \rangle</eqn>. Therefore, the average branching for a random network is <eqn>\langle d \rangle = \lambda</eqn> and, given that <eqn>n \approx \lambda^L</eqn> ignoring cycles, we obtain the same approximation for the average path length in random networks:
</text>

<equation>
L \approx \frac{ \log n }{ \log \lambda } 
</equation>


<text>
Link efficiency... Random networks are highly efficient users of links because of the small-world effect.
Because a small amount of randomness in any network injects a major drop in average path length, randomness results in a large jump in link efficiency! </text>

</document>



<document>
<tag>random-poisson-clustering</tag>
<title>Clustering coefficients</title>


<text>
- Cluster coefficients are expected to be small in comparison with more structured networks.

As the average path length decreases because of an increase in the density of links in a
random network, the cluster coefficient does the opposite: it should increase because more links means that
triangular subgraphs are more likely to form (recall that clusters
are created by linking adjacent neighbors to form triangular subgraphs)
</text>


<text>
When density equals 100%, the network is no longer random—it is complete. In a complete network, every node is connected to every other node, so the cluster coefficient of every node is 1.0. Alternatively, as a network becomes sparse, its cluster coefficient decreases—
ultimately to zero.
</text>

<text>
Watts and Strogatz (Watts, 1998) derived a theoretical estimate of random network cluster coefficient, as follows
</text>

<equation>
CC(i) = \frac { 2 c_i } { d_i ( d_i - 1 ) }
</equation>

<text>
Since edges are independent, they all have the same probability
</text>

<equation>
c_i = p \frac { d_i ( d_i - 1 ) } {2}
</equation>

<text>
Hence, 
</text>

<equation>
CC = p = \frac { \lambda } { n } = \rho
</equation>

<text>
that is, clustering is directly proportional to the number of (random) links added to the network. In fact, the clustering coefficient of a random network increases linearly with its density (both have equal values). 

Typically, the clustering coefficient of random graphs is small.
</text>

<text>
By definition, cluster coefficient of average node v with mean degree <eqn>\lambda</eqn> is:
</text>

<equation>
CC(v) = \frac { 2c } { \lambda (\lambda -1 ) }
</equation>

<text>
where c is the number of links among adjacent nodes. Using a mean-field approach to estimation, 
the number of links among adjacent nodes (forming triangular subgraphs) is <eqn>\lambda</eqn> things taken two at a time:
</text>

<equation>
{\lambda \choose 2} = \frac { \lambda! } { 2 (\lambda - 2 )! } = \frac { \lambda (\lambda -1) } { 2 }
</equation>

<text>
for fully-connected networks, but just the following for sparse networks:
</text>

<equation>
\rho {\lambda \choose 2} = \rho \frac { \lambda (\lambda -1) } { 2 }
</equation>

<text>
But, since the proportion of links present in a cluster equals the network's overall density, the mean-field approximation for <eqn>c</eqn> is also
</text>

<equation>
c = \rho {\lambda \choose 2} = \rho \frac { \lambda (\lambda -1) } { 2 } = \frac { \rho  \lambda (\lambda - 1 ) } { 2 }
</equation>

<text>
Substituting in the expression for the clustering coefficient, we obtain:
</text>

<equation>
CC(v) = \frac { 2c } { \lambda (\lambda -1 ) } =  \rho = \frac { \lambda } { n }
</equation>

<text>
i.e. the CC for an infinite sparse random network approaches zero
</text>

<text>
From this result, we can estimate the number of triangles in a random network:
</text>

<equation>
C_3 = \frac { \lambda^3} { 6 }
</equation>

<text>
since <eqn>C_3</eqn> is one third of the clustering coefficient times the number of connected triples in the network. The number of connected triples of nodes is <eqn>n(\langle \lambda^2 \rangle - \langle \lambda \rangle)/2</eqn> and, for Poisson distributions, their first and second moments are related: <eqn>\langle \lambda^2 \rangle - \langle \lambda \rangle = \langle \lambda \rangle^2</eqn>. Surprisingly, this number does not depend on the network size.
</text>

<text>
Similarly, we could also estimate the number of cycles of a given length, which does not depend on the network size either:
</text>


<equation>
C_L = \frac { \lambda^L} { 2L }
</equation>

<text>
when the cycle length <eqn>L</eqn> is smaller than the network diameter (expected to be of the order of <eqn>\log n</eqn>). When the network size increases, the neighborhood of a node is almost guaranteed to contain no cycles. However, it should also be noted that there might be many cycles whose length exceeds the network diameter (<eqn>\log C_L \sim n</eqn> when <eqn>L \gg \log n</eqn>). Hence, random networks are said to be <term>locally tree-like networks</term>. 
</text>



</document>


<document>
<tag>random-poisson-centrality</tag>
<title>Centrality measures</title>

<text>
(X) Motivation: Social network analysis is often interested in the power of actors (nodes) in a social network. But the definition of “power” often varies from one application to another. It might be related to how many links connect an actor to others (hub analysis), how far away an actor is from all other actors (radius), or the intermediary position of an actor within the network (betweenness)? Sometimes, as in Milgram's experiment, diameter and radius are of interest, radius might be more appropriate for epidemiology, and betweenness may be more appropriate for analyzing network dynamics.
</text>

<!-- Radius -->

<text>
Diameter and radius decrease as network density increases, as expected.

Surprisingly, radius can be larger than average path length.
since the radius of the central node is not the minimum of shortest paths, but rather the minimum of longest paths!
(the number of hops from the central node to the most remote node)

The average path, on the contrary, is an
average over short and long paths. Therefore, it is entirely possible for the average
path length to be smaller than the radius of the central node. The radius of a
network is very large if only one pair of nodes is separated by a long distance.
</text>

<!-- Betweenness -->

<text>
A node is considered important to other nodes if many shortest paths connecting the other nodes run through the close node.

Recall that betweenness of node w is defined as the number of direct paths from every other node u to every other node v running through node w.

... a connectivity metric.
</text>

<text>
Diameter and radius plummet with increase in density, but betweenness does not!

This counterintuitive result is shown to be a consequence of two forces acting simultaneously
on the structure of direct paths in a random network

1) increasing the number of links also increases the number of paths through a typical node

2) increasing density also decreases the average path length, which decreases betweenness
</text>


<text>
The betweenness of an average node increases with density, up to a maximum value: due to the increase in number of links because the number of paths increases

And then it decreases with density (eventually,
the increase in links produces shorter paths because of the small-world effect. These
shorter paths bypass the (larger number of) alternative and longer paths. Short
paths short-circuit the longer ones, which tend to decrease the number of paths
running through a typical node. At some density point, the number of paths
running through a typical node starts to decline)

In other words, betweenness increases with an increase in the number of links,
but is soon overwhelmed by the rapid drop in length of shortest paths, due to the
small-world effect.
</text>

<text>
two factors: average path length and number of shortestt paths.
</text>

<text>
We estimate the number of paths passing through a typical node using a mean-field approach. Node v has an average of \lambda links attached to it.
Each link connects v to approximately \lambda^D other nodes, where D is the diameter of the network. Therefore, the number of paths through node v should be proportional to \lambda^D. But we know that increases in density lead to shortcuts around node v. The number of paths through v decreases with density, according to (1-\rho). As an approximation, we get betweenness(random) \sim (1 - \rho) \lambda^D, which supports the conjecture
that average betweenness is a function of sparseness (1-density) and average path length.
</text>

<text>

DRAFT...

Apparently, there is a somewhat linear relationship between betweenness and path length:
...
betweenness(random) \sim L(random) = D 

D = \frac{\log n}{\log \lambda}
D = \frac{ \log n - \log \lambda }{ \log(\lambda-1) + 1 } 

\lambda = \rho n
...
betweenness(random) = \frac { \log n } { \log (n\rho) } = \frac {1}{\rho}
...

In addition, the number of paths running through the closest node varies according to #paths(intermediary)  \sim \ell
</text>


<text>
- Betweenness/closeness properties are small in comparison with other structured networks.
</text>


</document>

</document>



<!-- Giant components -->

<document>
<tag>random-phase-transition</tag>
<title>The giant component</title>


<text>
Giant component...

i.e. starting with a random graph that may contain multiple components, observe how a giant component forms. 
- uniformly random network so p and density are related by pm = n(n - 1)(density/2). 
- Adding links at a constant rate increases density at a constant rate. At some point, the network is dense enough to form a giant component.
=  a giant component "suddenly" forming as density increases.
</text>

<text>
When the average degree of a random network approaches 0, the random graph is disconnected
When <text>\lambda</text> barely exceeds 1, the giant component suddenly emerges, a fact already observed by Solomonoff and Rapoport in 1951.  
</text>

<text>
What is the probability that a graph with n nodes and connection probability p is connected?
Some of these properties appear suddenly, at a threshold p_c(N)

- If <eqn>p = c/n</eqn> with <eqn>c &lt; 1</eqn> (i.e. <eqn>p &lt; 1/n</eqn>), the graph contains only isolated trees (with a small number of cycles).
- At <eqn>p = c/n</eqn> with <eqn>c = 1</eqn>, a giant component appears (i.e. a large connected component containing a significant fraction of nodes).
</text>

<text>
A component is considered to be giant when its number of nodes grows proportionally to the network size. 

The appearance of the giant component is the most important structural change in a network, since many of its features will be affected by the existence of such a giant component.
</text>


<!-- Phase transitions -->

<comment>
<title>Phase transitions</title>

<text>
L: The same fundamental phenomenon observed in different fields: Phase transition 
occurs in a variety of real-world phenomena in the physical world 

1) matter changes from a solid state to a liquid, from liquid to gas, and so on. 
= the transition of a liquid to a solid
= a liquid “suddenly” converting to a solid, 
= The “crystallization” of a liquid into a solid, 
= between liquid and solid phases 

2) The Ising effect, for example, explains the phase transition that occurs when inert iron converts into magnetic iron. 
= phase transition is claimed to explain the Ising effect—the transition to magnetic polarity in ferrous materials. 
= inert iron to magnetic iron.
= nonmagnetic iron “suddenly” becoming magnetic—the so-called Ising model in statistical physics. 
= transition from nonmagnetic to magnetic iron
= between nonmagnetic and magnetic phases.

</text>

<text>
Such abrupt changes near a crossover point may also help explain other
phenomena in biology (neurological modeling) and consumer behavior (fads and
products that suddenly “catch on”).
</text>

<text>
++ <cite>Albert and Barabasi 2002</cite> provide succinct surveys of transitions of interest in physical and biological science. 
</text>

<text>
 p* is the tipping point...
</text>

</comment>

<!-- Component sizes -->


<text>
When the average degree <eqn>\lambda</eqn> is greater than one, the network will contain a giant component and potentially many smaller connected components (as the ones that appear when there are more nodes than links). 
</text>

<text>
The size of the giant cluster approaches the size of the complete network as the average node degree increases: 

- Below this threshold, no giant component forms.

- Exactly at the critical point, all the biggest connected components are of size <eqn>n^{2/3}</eqn>, which is much smaller than <eqn>n</eqn> and much bigger than <eqn>\log n</eqn>.

- Above this threshold, a complex giant component forms (with a limited number of cycles, as you know from our study of the properties of random networks). 

	- Near the critical point, s = 2(\lambda-1)n

	- Even for very sparse networks, e.g. with <eqn>\lambda = 5</eqn>, the relative size of the giant component is above 99%. 

	- The graph becomes connected if <eqn>\lim_{n \rightarrow \infty} \frac{p}{\ln n / n} \rightarrow \infty</eqn>
</text>


<text>
The theoretical study of the size of connected components in random networks has shown that their size follows a power law <eqn>P(s) \sim s^{-5/2}</eqn> at the critical point, whereas their distribution has an exponential decay in their normal phase (many small connected components, mostly of one or two nodes) and also in their phase with a giant component.
</text>

<text>
What is the probability that a randomly-chosen node is in a connected component of <eqn>s</eqn> nodes? This probability <eqn>P'(s)</eqn>is proportional to <eqn>sP(s)</eqn>. At the critical point, <eqn>P'(s) \sim s^{-3/2}</eqn> and the average size <eqn>\langle s \rangle'</eqn> of the connected component diverges. The analytical form of such size is  <eqn>\langle s \rangle' \approx 1/ | \lambda - 1 |</eqn> and is analogous to the Curie-Weiss law for susceptibility in Physics.
</text>


<!-- Percolation theory -->

<comment>
<title>Percolation theory</title>

<text>
Condensed-matter physicists would establish an analogy with percolation clusters: remove some nodes from a lattice until it gets disconnected below some critical value (the percolation threshold). For them, the birth of a giant component is an example of a continuous phase transition (i.e. there is no abrupt change of the relative size of the giant component at the critical point).
</text>

<text>
In percolation theory, p* is called the percolation threshold.

L: e.g. Bond percolation is modeled as a two-dimensional grid with pm links. Links are constrained by a flat 2D surface, so each node can connect only along NEWS directions (=north, east, west, south); hence m is bounded by 4n, and so pm \leq 4n (density). Starting with p = 0, p increases until a giant connected component emerges. If it is possible to reach every other node from any node, the entire network is one giant component.

L: Materials also transition the other way—from structured to random. Consider a fully connected complete network with m = n((n-1)/2) links. This giant connected network represents a frozen cube of water or fully magnetized metal. Now, randomly select at link and remove it with probability (1-p). Test the network to see if it is still one giant component. Repeat the random link removal process until the giant component is no longer connected. At this point, pm links remain. What is the significance of p*, now? A percolation network is in its subcritical phase when p &lt; p*, and its supercritical phase when p &gt; p*. 

In certain restricted cases (Cayley graphs), p* = 1/n. 

</text>
</comment>


</document>



<!-- Real world -->

<document>
<tag>random-real-world</tag>
<title>Are real networks like random graphs?</title>

<text>
Empirical analysis: Determine L, CC and P(k) for a random graph with the same <eqn>n</eqn> and <eqn>\lambda</eqn>.

- Real networks have short distances like random graphs yet they show signs of clustering.
- Their degree distribution is often a power law.
</text>

<text>
e.g.
Internet: half a million routers with average degree 10 and CC=0.1. If it were a random network, CC \approx 10^{-5} <cite>Dorogotsev 2010</cite>

e.g. 
WWW <cite>Broder et al. 2000</cite> analyzed over 200 million servers in the Internet’s Webgraph, and discovered its giant component, called the giant strongly connected component (GSCC)... The Internet topology, however, is not random, but scale-free.

</text>

<text>
Therefore, our answer to the question that appears in the title of this section should be <q>not really</q>. Despite this apparently discouraging conclusion, random graphs are more than toy models. Among other things, they help us explain some of the properties that are observed in real-world networks. And they will also serve as the baseline benchmark against which we will compare more sophisticated network models.
</text>

</document>


<!-- Bibliographic notes -->


<document>
<tag>random-notes</tag>
<title>Bibliographic notes</title>

<text>
++ <cite>Erdos and Renyi 1959</cite> <cite>Erdos and Renyi 1960</cite>
</text>

<text>
++ <cite>Bornholdt and Schuster 2003</cite> <cite>Bollobas et al. 2009</cite>
</text>

<text>
++ Graph generation @ <cite>Skiena 2008</cite>, p. 460
</text>

<text>
++ Random graphs from Stanford Graphbase(GB_RAND)
</text>


</document>


</document>
